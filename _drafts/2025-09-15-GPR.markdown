---
layout: post
title:  "GPR Notes UPC"
date:   2025-09-15 12:00:00 +0200
preview: "/images/gpr/gpr.png"
categories: post
permalink: post/GPR
---

This are some of my notes for Geometry Processing. 
<!-- end-abstract -->

<!-- index -->
* Do not remove this line (it will not be displayed)
{:toc}

{% bibliography_loader _bibliography/ao_references.bib %}


# Context
-  Final Grade = 0.1 Class + 0.35 Lab + 0.4 Exam + 0.15 LabExam.

# Previous capacities
## vector spaces
## transformations
Linear transformations always preserve the origin {% equation_inline A0=0 %}

## change of bases
## eigenvalues and eigenvectors
`Eigenvectors` are special, non-zero vectors that do not change their direction when a linear transformation is applied to them, instead being scaled by a constant factor called an `eigenvalue`.

For {% equation_inline v %} to be an eigenvector of {% equation_inline A %}

{% equation id="for-v-to-be-eigenvector" %}
A v = \lambda v
{% endequation %}

with {% equation_inline  λ \in R %} and {% equation_inline v \ne 0 %} 

But the **why do we care?** is still fuzzy. Let’s make it concrete with examples from geometry processing and beyond.
1. Eigenvectors give special directions of a transformation

    Imagine you have a matrix {% equation_inline  A %} that transforms vectors. 
  
    In most directions, {% equation_inline  Av %} will change direction and length. But along eigenvectors, {% equation_inline  Av %} only changes length (scaling by {% equation_inline  \lambda %}).

    So:
      * Eigenvectors = “axes” of the transformation.
      * Eigenvalues = how much the transformation stretches/squashes along those axes.
    
    That’s the raw math meaning. Now — what do we do with that?

## SVD
Singular Value Decomposition (SVD) is a powerful matrix factorization technique that expresses any $m \times n$ matrix $A$ as the product $A = U \Sigma V^T$, where $U$ and $V$ are orthogonal matrices and $\Sigma$ is a diagonal matrix containing the singular values of $A$. SVD reveals the intrinsic geometric structure of the matrix, making it invaluable for applications such as dimensionality reduction, noise filtering, and solving linear systems, especially when $A$ is not square or is ill-conditioned.

To find the singular values of a matrix \(A\), you must compute the square roots of the positive eigenvalues of the matrix \(A^{T}A\) (or \(AA^{T}\)). The general process involves calculating \(A^{T}A\), determining its characteristic polynomial by setting the determinant of (\(A^{T}A-\lambda I\)) to zero, solving for the eigenvalues (\(\lambda \)), and then taking the square root of each positive eigenvalue to find the singular values of  A.

## differential geometry of curves and surfaces:
## C++

# Geometry processing pipeline, (reconstruction)

- point clouds -> alignment -> point cloud -> normal estimation -> point cloud +normal -> Reconstruction -> surface
- Loss in each step


# Surfaces 
## Linear Algebra
Linear algebra is a branch of mathematics that studies linear equations, linear functions, and their representations through vectors and matrices. It provides the foundational language and tools for analyzing systems where relationships are linear, meaning they can be described by equations of the form 
{% equation_inline a_1x_1 + a_2x_2 + \cdots + a_nx_n = b %}. 
Linear algebra is essential in many fields, including science, engineering, computer graphics, and machine learning, as it allows for the efficient manipulation and understanding of multidimensional data and transformations.


**Eigendecomposition** is a method of expressing a square matrix in terms of its eigenvalues and eigenvectors. For a matrix $A$, if it can be written as $A = V D V^{-1}$, where $V$ contains the eigenvectors and $D$ is a diagonal matrix of eigenvalues, then $A$ is said to be diagonalizable. This decomposition reveals the fundamental directions (eigenvectors) and scaling factors (eigenvalues) of the transformation represented by $A$. Eigendecomposition is useful in simplifying matrix operations and understanding the behavior of linear transformations in geometry processing, physics, and data analysis.

{% equation id="energy" %}
A \in R^{n \times n}
{% endequation %}

For future examples, let’s use the following $3 \times 3$ matrix as $A$:
{% equation %}
A = \begin{pmatrix}
2 & 1 & 0 \\
1 & 2 & 1 \\
0 & 1 & 2
\end{pmatrix}
{% endequation %}

The formula $Av = \lambda v$ means that when you apply the matrix $A$ (a linear transformation) to the vector $v$ (an eigenvector), the result is just the same vector $v$ scaled by the number $\lambda$ (the eigenvalue). 

In other words, $v$ is a special direction for $A$: instead of changing direction, $A$ only stretches or shrinks $v$ by the factor $\lambda$. The matrix $A$ does not "store" the eigenvalues; rather, the eigenvalues and eigenvectors are special solutions to this equation for a given $A$.

<!-- eigenvector: 
    
    Av=λv

    A is a linear transformation matrix
    v is a nonzero vector
    λ is a scalar.

    This equation says: when you apply   -->
<!-- 
v eigenvector of A <=> Av=lambda*v where v not equal zero and v belongs to R^{n} and lambda belongs to R and ||v||=1
Lambda eigenvector of A corresponding to v -->

<!-- Av = lambda v -> Av-lambda v = 0 -> Av -lambda Iv = 0 -> (A-lambdaI)v=0
A-lambdaI = 0 -->

We can rewrite the eigenvector equation step by step:
{% equation %}
Av = \lambda v \\
Av - \lambda v = 0 \\
Av - \lambda I v = 0 \\
(A - \lambda I)v = 0
{% endequation %}

To have a nontrivial solution ($v \neq 0$), the matrix $(A - \lambda I)$ must be singular (not invertible). This happens only when its determinant is zero:
{% equation %}
\det(A - \lambda I) = 0
{% endequation %}
This is called the characteristic equation. Solving it gives the eigenvalues $\lambda$ of $A$. For each eigenvalue, you can then find the corresponding eigenvectors $v$ by solving $(A - \lambda I)v = 0$.

<!-- 
A symetric -> lambda' s belongs to R
              v' s belongs to R^{n}
              v' s linearly independent
              v orthogonal (i can get an orthonormal system?)

These are:
Characteristic equation of A
characteristic polynomial of A 
-->

The **characteristic polynomial** of a matrix $A$ is the polynomial you get when you compute $\det(A - \lambda I)$, where $I$ is the identity matrix and $\lambda$ is a variable. It is called a polynomial because, for an $n \times n$ matrix, expanding the determinant gives a degree $n$ polynomial in $\lambda$. The roots of this polynomial are the eigenvalues of $A$. In summary:

{% equation %}
	ext{Characteristic polynomial:}\quad p(\lambda) = \det(A - \lambda I)
{% endequation %}
Solving $p(\lambda) = 0$ gives all the eigenvalues of $A$.


#### 1. Subtract $\lambda$ from each diagonal entry to get $A - \lambda I$: 
  {% equation %}
  A - \lambda I = \begin{pmatrix}
  2 - \lambda & 1 & 0 \\
  1 & 2 - \lambda & 1 \\
  0 & 1 & 2 - \lambda
  \end{pmatrix}
  {% endequation %}

#### 2. Compute the determinant and set it to zero: 
  {% equation %}
  \det(A - \lambda I) = 
  \begin{vmatrix}
  2 - \lambda & 1 & 0 \\
  1 & 2 - \lambda & 1 \\
  0 & 1 & 2 - \lambda
  \end{vmatrix}
  = 0
  {% endequation %}

#### 3. Expand the determinant: 
   {% equation %}
  (2-\lambda)\left[(2-\lambda)(2-\lambda) - 1\cdot1\right] - 1\left[1(2-\lambda) - 1\cdot0\right]
  {% endequation %}
  which simplifies to:
  {% equation %}
  (2-\lambda)\left[(2-\lambda)^2 - 1\right] - (2-\lambda)
  {% endequation %}
  {% equation %}
  = (2-\lambda)\left[(2-\lambda)^2 - 1\right] - (2-\lambda)
  {% endequation %}
  {% equation %}
  = (2-\lambda)\left[(2-\lambda)^2 - 1 - 1\right]
  {% endequation %}
  {% equation %}
  = (2-\lambda)\left[(2-\lambda)^2 - 2\right]
  {% endequation %}


#### 4. Set this equal to zero and solve for $\lambda$:
  {% equation %}
  (2-\lambda)\left[(2-\lambda)^2 - 2\right] = 0
  {% endequation %}

So the eigenvalues are the solutions to {% equation_inline 2-\lambda = 0$ and $(2-\lambda)^2 - 2 = 0 %}.

- $2-\lambda = 0 \implies \lambda_1 = 2$
- $(2-\lambda)^2 - 2 = 0 \implies (2-\lambda)^2 = 2 \implies 2-\lambda = \pm\sqrt{2} \implies \lambda_2 = 2+\sqrt{2},\ \lambda_3 = 2-\sqrt{2}$

So the eigenvalues are $\boxed{2,\ 2+\sqrt{2},\ 2-\sqrt{2}}$.


### Obtain Eigenvectors
Let’s work this one out step by step. You’ve already correctly found the eigenvalues of

{% equation %}
A = \begin{bmatrix}
2 & 1 & 0 \\
1 & 2 & 1 \\
0 & 1 & 2
\end{bmatrix}
{% endequation %}

The eigenvalues are:

{% equation %}
\lambda_1 = 2, \quad \lambda_2 = 2+\sqrt{2}, \quad \lambda_3 = 2-\sqrt{2}.
{% endequation %}

Now let’s find the eigenvectors.

---

#### 1. General method

For each eigenvalue $\lambda$, solve

{% equation %}
(A - \lambda I) v = 0
{% endequation %}

for $v \neq 0$.

---

#### 2. Eigenvector for $\lambda_1 = 2$

{% equation %}
A - 2I =
\begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 0
\end{bmatrix}
{% endequation %}

So we need to solve:

{% equation %}
\begin{cases}
y = 0 \\
x+z = 0 \\
y = 0
\end{cases}
{% endequation %}

That gives $y=0$, $z=-x$.
So an eigenvector is

{% equation %}
v_1 = \begin{bmatrix} 1 \\ 0 \\ -1 \end{bmatrix}
{% endequation %}

---

#### 3. Eigenvector for $\lambda_2 = 2+\sqrt{2}$

{% equation %}
A - (2+\sqrt{2})I =
\begin{bmatrix}
-\sqrt{2} & 1 & 0 \\
1 & -\sqrt{2} & 1 \\
0 & 1 & -\sqrt{2}
\end{bmatrix}
{% endequation %}

System:

{% equation %}
\begin{cases}
-\sqrt{2}x + y = 0 \quad \Rightarrow \; y = \sqrt{2} x \\
x - \sqrt{2} y + z = 0 \\
y - \sqrt{2} z = 0
\end{cases}
{% endequation %}

From the first: $y = \sqrt{2}x$.
From the third: $z = y / \sqrt{2} = x$.
So eigenvector is

{% equation %}
v_2 = \begin{bmatrix} 1 \\ \sqrt{2} \\ 1 \end{bmatrix}
{% endequation %}

---

#### 4. Eigenvector for $\lambda_3 = 2-\sqrt{2}$

{% equation %}
A - (2-\sqrt{2})I =
\begin{bmatrix}
\sqrt{2} & 1 & 0 \\
1 & \sqrt{2} & 1 \\
0 & 1 & \sqrt{2}
\end{bmatrix}
{% endequation %}

System:

{% equation %}
\begin{cases}
\sqrt{2}x + y = 0 \quad \Rightarrow \; y = -\sqrt{2}x \\
x + \sqrt{2}y + z = 0 \\
y + \sqrt{2} z = 0
\end{cases}
{% endequation %}

From the first: $y = -\sqrt{2}x$.
From the third: $z = -y/\sqrt{2} = x$
So eigenvector is

{% equation %}
v_3 = \begin{bmatrix} 1 \\ -\sqrt{2} \\ 1 \end{bmatrix}
{% endequation %}

---

#### 5. Final result

The eigenpairs are:

* $\lambda_1 = 2 \;\;\Rightarrow v_1 = [1,0,-1]^T$
* $\lambda_2 = 2+\sqrt{2} \;\;\Rightarrow v_2 = [1,\sqrt{2},1]^T$
* $\lambda_3 = 2-\sqrt{2} \;\;\Rightarrow v_3 = [1,-\sqrt{2},1]^T$

---

✅ Notice: since $A$ is **symmetric**, the eigenvectors are mutually orthogonal (check dot products — they’re zero).

---















<!-- Av_i =lambda_i v_i
A*V=V*D, V=[[...],[v1,v2,...,vn],[...]].T
A*[v1,v2,...,vn]=[col.Av_1 col.Av_2 ... colAv_n]
V*V.T = I = V.T*V
V*D = [col.lambda_1*v_1 ... col.lambda_n*v_n]
A=V*V.T=V*D*V.T
A=V*D*V.T

example:
with Av=λv and |A-λI|=0
A = (3 1)
    (1 3)

Solve λ, given the values of λ solve (A-λ_1*I)*v_1=0

what happens when A is (1 4)
                       (3 2) -->
## Fitting a line (least squares)
E(l_1,P_i)= y_i-ax_i-b
E(P, l) = sumatory{n, i=i}(y_i-ax_i-b)^2
?what is the derivative of this formula for? with derivative of "a" and "b" we can make a system of equations?   

we can somehow extract the least square error with matrix operations
E(x) = ||Ax-b||^2 = <Ax-b, Ax-b> = (Ax-b).T*(Ax-b) = (x.T*A.T-b.T) (Ax-b)... = x.T*A.T*A*x-2x.T*A.T*b+b.T*b

we can now get the gradient of E -> ∇E=0
∇E = 2A.T*Ax-2A.T*b=0 -> A.T*Ax = A.T*b


## Analysis (curvatures)
## Simplification

- Lagrange multiplier & lagrange functions

  Lagrange multipliers and Lagrange functions are mathematical tools used to find the maximum or minimum of a function subject to one or more constraints. Instead of searching for extrema in the whole space, they allow us to optimize a function while ensuring that certain conditions (constraints) are always satisfied. The method introduces new variables (the multipliers) and combines the original function and the constraints into a single function, called the Lagrangian. By finding the critical points of this Lagrangian, we can solve constrained optimization problems efficiently.

  To simplify the process we can eve use matrix notation. You want to minimize a quadratic error function $E(n,d)$ with n = normal vector and d=distance from the origin of an hyperplane.

{% equation %}
  E(n,d)
{% endequation %}

  * n = vector to optimize (normal to plane).
  * d = scalar shift.

Given points 
$p_i$, the signed distance from a point to the plane is:
{% equation %}
  dist(p_i,n,d) = n \cdot p_i + d
{% endequation %}
If we want the best-fit plane (least squares), we minimize:
{% equation %}
  E(n,d) = \sum{(n \cdot p_i + d)^2}
{% endequation %}
That’s the quadratic error function.

If you try to minimize E(n,d) directly, the trivial solution is n=0,d=0, which is meaningless.

We need a constraint:
{% equation %}
  ||n||^2=1
{% endequation %}
to ensure the normal is a unit vector.

We now minimize:
{% equation %}
L(n,d,λ)=E(n,d)−λ(∥n∥^2−1)
{% endequation %}
* Here, λ is the Lagrange multiplier.
* Taking derivatives w.r.t. n,d,λ gives a system of equations.
* Solving it gives the optimal plane normal n and offset d.


The derivatives are:

* With respect to $n$:
{% equation %}
\frac{\partial L}{\partial n} = 2 C n + 2 d c - 2 \lambda n
{% endequation %}
  (where $C$ and $c$ are the covariance matrix and mean vector, depending on how $E(n,d)$ is written)

* With respect to $d$:
{% equation %}
\frac{\partial L}{\partial d} = 2 c^T n + 2 d N
{% endequation %}
  (where $N$ is the number of points)

* With respect to $\lambda$:
{% equation %}
\frac{\partial L}{\partial \lambda} = - (\|n\|^2 - 1)
{% endequation %}

Setting these derivatives to zero gives the system of equations whose solution yields the optimal $n$ and $d$.

<!-- 
the professor said "solutions to opt f(x) s.t g(x)=0
are the critical points of Lagrange"

It is doing partial derivative of lambda, x and y (not sure why)
We are given a function for a line 

l = ax + by + c = 0
but also an alternative version which can  also represent spheres. This is a bit confusing
h^t p + d = 0
||n|| = 1
d(p,l)=n^T p + d
E(l) = sum(n, i=1) d(p_i, l)^2 = sum (n^T p_1 + d)^2
We also can understand Error as 
E(n,d); OPT E(n,d) s.t. ||n||=1

We end up with n^T C n - lambda(n^T n -1) where C is a covariance matrix

This means 
derivative lagrange / derivative n = 2*C*n-2*lambda*n=0 which means C*n = lanbda*n
n will become Eigenvectors of C
lambda will become eigenvalues of C

E(n,d) = n^T * C * n = n^T * lambda * n = lambda*n^T*n = lambda

n eigenv of C with smallest eigenv lambda -->

## getting points
Having multiple point clouds knowing the camera position allows us to know the geometry of the photographed object

### Alignment (registration)

Given two point clouds, {% equation_inline P = \{P_i\} \text{ for } 1 \leq i \leq n %} and {% equation_inline Q = \{Q_i\} \text{ for } 1 \leq i \leq n %}, we seek a transformation—typically a rotation and translation—that aligns $Q$ to $P$.

The goal is to find a rotation matrix {% equation_inline R %} and a translation vector {% equation_inline t %} such that:

{% equation %}
P_i \approx R Q_i + t \quad \text{for } 1 \leq i \leq n
{% endequation %}

This process is known as point cloud registration or alignment.

### SVD (singular value decomposition)
{% equation_inline A \in \mathbb{R}^{n \times n} %}
Eigendecomposition:
{% equation %}
A = V D V^T
{% endequation %}
where
* {% equation_inline V %} contains the eigenvectors of {% equation_inline A %},
* {% equation_inline D %} is a diagonal matrix of eigenvalues,
* {% equation_inline V^T %} is the transpose of {% equation_inline V %}.

Singular Value Decomposition (SVD):
{% equation_inline A \in \mathbb{R}^{m \times n} %}
{% equation %}
A = U \Sigma V^T
{% endequation %}
where
* {% equation_inline U \in \mathbb{R}^{m \times m} %} is an orthogonal matrix,
* {% equation_inline V \in \mathbb{R}^{n \times n} %} is an orthogonal matrix,
* {% equation_inline \Sigma \in \mathbb{R}^{m \times n} %} is a diagonal matrix with singular values.

Orthogonal matrices ({% equation_inline U %} and {% equation_inline V %}) represent rotations or reflections.


{% equation %}
\Sigma = \operatorname{diag}(\alpha_1, \alpha_2, \ldots, \alpha_{\min(m,n)})
{% endequation %}
where each diagonal entry {% equation_inline \alpha_i %} is a singular value of {% equation_inline A %}, and {% equation_inline \alpha_i \geq 0 %}.


Given a linear system {% equation_inline Ax = b %}, where {% equation_inline A \in \mathbb{R}^{r \times n} %}, we can use the Singular Value Decomposition (SVD) to solve for {% equation_inline x %}. If {% equation_inline A = U \Sigma V^T %}, then:

{% equation %}
U \Sigma V^T x = b
{% endequation %}

Multiplying both sides by {% equation_inline U^T %}:

{% equation %}
\Sigma V^T x = U^T b
{% endequation %}

Let {% equation_inline y = V^T x %}, so:

{% equation %}
\Sigma y = U^T b
{% endequation %}

Solving for {% equation_inline y %}:

{% equation %}
y = \Sigma^{-1} U^T b
{% endequation %}

Then, recovering {% equation_inline x %}:

{% equation %}
x = V y = V \Sigma^{-1} U^T b
{% endequation %}

This expresses the solution to {% equation_inline Ax = b %} using SVD.


### Simpler Alignment
Given two point clouds:
- {% equation_inline P = \{P_i\} \text{ for } 1 \leq i \leq n %}
- {% equation_inline Q = \{q_i\} \text{ for } 1 \leq i \leq n %}

Assume:
- Both sets have the same number of points ($n$).
- Each $P_i$ corresponds to $q_i$.

We seek a rotation matrix {% equation_inline R \in \mathbb{R}^{3 \times 3} %} and a translation vector {% equation_inline t \in \mathbb{R}^3 %} such that:

{% equation %}
P_i \approx R q_i + t
{% endequation %}

The alignment error function is:

{% equation %}
E(R, t) = \sum_{i=1}^{n} \| P_i - R q_i - t \|^2
{% endequation %}

To solve for $t$:
- Compute the centroids of $P$ and $Q$:

{% equation %}
P' = \frac{1}{n} \sum_{i=1}^{n} P_i
{% endequation %}
{% equation %}
q' = \frac{1}{n} \sum_{i=1}^{n} q_i
{% endequation %}

- The optimal translation is:

{% equation %}
t = P' - R q'
{% endequation %}

So, the aligned points satisfy:

{% equation %}
P' = R q' + t
{% endequation %}

### Steps
1. Compute centroids {p', q'}
2. compute centroids adjusted
    (p'=p_i-p') q`= q_i-q'

3. S = QP.T = (q'1, q'2, ..., q'n) * ([p'1],[p'2],[...],[p'n])
4. S = U*covariancematrixsymbol*V.T
5. R=V*U.T
6. t = p'-R*q'

### Iterative closest point (ICP)
1. compute correspondence
  For every q_i fing closest p_k
2. Use simpler alignment (SVD)
   * Discard Points in P without correspondence
   * Replicate points in P multiply selected.
  From this points we get a P' that will be used temporarily
  Get R,t 
3. Apply R_it  [unknown symbol] -> Q q_i' <- Rq_1 + t
4. Repear from 1
  

### Stop criteria
1. norm(R-I)_f < epsilon where ||t||<e
   1. Correspondance constant
2. Maximum # iterations
   1. need initial rough alignment (there are better alternatives in the literature)

### Compute border point status
One of the posible problems is that point clouds are apart and the closest point for each other is a single point from P for all Q points. 

One solution could be to remove all overlapint points. The single point in P that corresponds to multiple Q points is called border point on P.
1. how do we detect border points and overlapped points?
  We will recognise border points by checking the size of the angle that does not posses any point.

For every point P_i:
  1. compute K-NN(Nearest Neighbours) of P_i -> {Pij} 1<=j<=k
  2. PCA -> Eigenvectors(v1,v2,v3==normal) + P_i == local frame (local coord system)
  3. (xij, yij, zij) = ((Pij -Pi)*V1, (Pij-pi)*V2, (Pij-pi)*V3)
  4. Discard Z'ij 
  5. Compute direction by taking the angle alpha_{ij}=atan2(y'_ij, x'ij)
  6. sort alpha_{ij}
  7. Differenes of adjacent pairs
     1. we call them deltaSymbol alpha_{ij}
  8. max(delta alpha_{ij}) >=B <=> P_i border point

(parameters -> K,B(Beta))

### Multiple Scans
Overlap -> 1st step of ICP -> compute correspondence
Overlap == Percentage of discarded points of Q
1. Select 2 unaligned clouds
2. Perform ICP on them result(successfull | fail)
   1. successfull == ||R-I||_f means ||t||=0 -> fuse the 2 clouds
   2. fail == success

### Solving linear systems
Ax=b, A belongs R^{nxn}, xi b belongs to R^n
Apply solvers
* Direct Performs op's to solve exactly (except machine precision)
* Iterative start with x0 estimation and then refine estimation.

### Properties and requirements to how we want to resolve the system
* Required performance
* Reuqired accuracy
* Limits for storage
* Problem: Dimensional n (A belongs to R^{nxn}), n > 10000 -> LARGE
* Sparsity of A == Percentage of O's in A (sparse|dense)
* symetry of A
* Positive definiteness of A
  * InvertedASymbol=/0 -> x.T * A * x > O <=> A is positive definite

### decompositions
* Direct LU decomposition -> A=LU 
  * L== lower triangular
  * U == Upper triangular
  * Ax=b -> LUx = b -> L_z = b, V_x=z
* QR decomposition A=QR -> R==Uper triangle, Q == Orthogonal
  * Ax=b -> QRx=b <=> Rx=z
  * z = Q.T*b == Qz=b 
* Cholesky
  * A is symetric and positive definite
  * A = LL.T / L==lower triangular

#### interactive
* Conjugate gradient (CG) A symetric and positive definite
* Biconjugate gradient (Bi-CG) 


## Smoothing (remove noise)
## Remeshing
## Parametrization
## Model repair
## Editing and reformation
## Synthesis
## Symetry detection

# Session 3
in general is more complex to this, but this works for most problems to choose a solver
1. Decide the size of the system. Depends a little bit on the system. size is  a convention made by people, not a mathematical solution
   1. large: means matrix is sparse. 
      1. A is symetric and positive matrix, then we can use CG
      2. if not, Bi-CG
      3. if it is large and dense: 
         1. Transform into sparse
         2. Specific solver for the problem we are going to solve.
         3. Brute force approach: highly parallel solver + hardware
   2. small: A symm+pos.det->cholesky
      1. other options QR & LU decomp.

## reconstruction
Reconstruction
      {p1 belongs to real number R^3}1<=i<=n and {n_i belongs to real number R^3}1<=i<=n. -> reconstructor -> surface (triangle mesh). For all points that belong to the surface face, q belongs to U of size n, Sphere(pi,epsilon) =>Q={q_i} p-dense w.r.t. S

      P={p_i}, pi=q_i+e_i, q_i belongs to S

      ||e_i||<& => P is delta-noisy
      If P is delta-noisy w.r.t. Q set inside S set, Q is delta-dense w.r.t S

      Therefore P is epsilon-dense & delta-noisy w.r.t. S

What we usually do is:
  - Ponit cloud -> reconstruct -> implicit function == f D->R where D intersects R^3 -> isosurface extraction -> triangle mesh
    - f(p) = 0  -> P is S
    - f(p)>0 -> P outside S
    - f(p)<0 -> p inside S

## algorithms for reconstructions
- A_inverted_symbol _i where 1<=i<=n f(p_i)=0 -> interpolatory

- A_inverted_symbol _i where 1<=i<=n f(p_i) close_to 0 -> approximative

* Hoppe et al. qa
  This is NOT a smooth function

  f(p)
    i <- Index of p_i closest to p.
    z <- p-((p-p_i)*n_i)*n_i
    if distance (z_i,p_i)<= delta+f then f(p) <-(p-p_i)*n_i
    else f(p) <- undefined

  * problems:
    * having f(p) undefined means we are going to get holes
    * Discontinuities: all this algorithm is interpolatory, these methods are usually not desired
* Moving least squares:
  This is a smooth function. Approximative algorithm. No holes, no quality for empty space.

  f_i(p) = (p-p_i)*n_i == bump function

  f(p)=sum(n, i=1) W_i(p) f_i(p) / sum(n, i=1) W_i(p)

  for the weight function W_i(p) we can choose whatever we see fit. W_i(p) = exp(-||p-p_i||^2 / euro_symbol^2)/A_i where A_i is #points closest to p than euro_symbol
    Asks for p to be euro_symbol-dense

* Radial Basis Functions
  Prior information, usually smooth prior
  The sumation of the "n" bump functions pases through some points but not necessarily over others. f(p)=c_1 * f_1(p) + c_2 * f_2(p) - c_3 f_3(p)

  f(p_1)=f(p_2)=f(p_3) = 0


  (p_i,v_i) with   1<=i<=n
    f(p_i)=v_i
    f(p)=sum(n,i=1) C_i f_i(p)
    f_i(p)=phi(||p-p_i||)
    phi(r)=exp(-r^2/2c)

    f(p_i)=v_i=sum() c_i phi(||p-p_i||)

      n equations 
      c_i = n variables
    
    A*c = v | A = [[phi(||p_1-p_1||), ..., phi(||p_n-p_n||)] [...] [phi(||p_1-p_n) ... phi(||p_n-p_n)]]

  * problems
    * As we have defined v=0=> C=0
      The solution to this:
      We add syntetic points. For every point p_i we generate a p_i^+ equal to p_i+d*n_i

      f(p_i^+) = d
      f(p_i^-) = -d

      now the problem is that we increase the size of the point sets 3 times on each direction, the result is 9 times bigger.

      A*c=v becomes unstable as n increases.
        Regularization A' = A+lambda*I
        (A+lambda*I)*c=v

      Interpolatory => Approximate

    * In case we have a large point set we cant triplicate the size problem.
      * compact support
        * phi(r) = {exp(-r^2/2c^2),      r<3c}
                   {0,              otherwise} 

* Poisson Surface Reconstruction
  It has a set of points and normals and tries to solve the indicator function where 0 is outside and 1 is inside

<!-- You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-title.MARKUP`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `MARKUP` is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:

{% highlight ruby %}
def print_hi(name)
  puts "Hi, #{name}"
end
print_hi('Tom')
#=> prints 'Hi, Tom' to STDOUT.
{% endhighlight %}

Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/ -->
