---
layout: post
title:  "GPR Notes UPC"
date:   2025-09-15 12:00:00 +0200
preview: "/images/gpr/gpr.png"
categories: post
permalink: post/GPR

published: false

---

This are some of my notes for Geometry Processing. 
<!-- end-abstract -->

<!-- index -->
* Do not remove this line (it will not be displayed)
{:toc}

{% bibliography_loader _bibliography/ao_references.bib %}


# Context
-  Final Grade = 0.1 Class + 0.35 Lab + 0.4 Exam + 0.15 LabExam.

# Previous capacities
## vector spaces
## transformations
Linear transformations always preserve the origin {% equation_inline A0=0 %}

## change of bases
## eigenvalues and eigenvectors
`Eigenvectors` are special, non-zero vectors that do not change their direction when a linear transformation is applied to them, instead being scaled by a constant factor called an `eigenvalue`.

For {% equation_inline v %} to be an eigenvector of {% equation_inline A %}

{% equation id="for-v-to-be-eigenvector" %}
A v = \lambda v
{% endequation %}

with {% equation_inline  λ \in R %} and {% equation_inline v \ne 0 %} 

But the **why do we care?** is still fuzzy. Let’s make it concrete with examples from geometry processing and beyond.
1. Eigenvectors give special directions of a transformation

    Imagine you have a matrix {% equation_inline  A %} that transforms vectors. 
  
    In most directions, {% equation_inline  Av %} will change direction and length. But along eigenvectors, {% equation_inline  Av %} only changes length (scaling by {% equation_inline  \lambda %}).

    So:
      * Eigenvectors = “axes” of the transformation.
      * Eigenvalues = how much the transformation stretches/squashes along those axes.
    
    That’s the raw math meaning. Now — what do we do with that?

## SVD
Singular Value Decomposition (SVD) is a powerful matrix factorization technique that expresses any {% equation_inline m \times n %} matrix {% equation_inline A %} as the product {% equation_inline A = U \Sigma V^T %}, where {% equation_inline U %} and {% equation_inline V %} are orthogonal matrices and {% equation_inline \Sigma %} is a diagonal matrix containing the singular values of {% equation_inline A %}. SVD reveals the intrinsic geometric structure of the matrix, making it invaluable for applications such as dimensionality reduction, noise filtering, and solving linear systems, especially when {% equation_inline A %} is not square or is ill-conditioned. 

To find the singular values of a matrix {% equation_inline A %}, you must compute the square roots of the positive eigenvalues of the matrix {% equation_inline A^{T}A %} (or {% equation_inline AA^{T} %}). The general process involves calculating {% equation_inline A^{T}A %}, determining its characteristic polynomial by setting the determinant of ({% equation_inline A^{T}A-\lambda I %}) to zero, solving for the eigenvalues ({% equation_inline \lambda %}), and then taking the square root of each positive eigenvalue to find the singular values of  A.

## differential geometry of curves and surfaces:
## C++

# Geometry processing pipeline, (reconstruction)

- point clouds -> alignment -> point cloud -> normal estimation -> point cloud +normal -> Reconstruction -> surface
- Loss in each step


# Surfaces 
## Linear Algebra
Linear algebra is a branch of mathematics that studies linear equations, linear functions, and their representations through vectors and matrices. It provides the foundational language and tools for analyzing systems where relationships are linear, meaning they can be described by equations of the form 
{% equation_inline a_1x_1 + a_2x_2 + \cdots + a_nx_n = b %}. 
Linear algebra is essential in many fields, including science, engineering, computer graphics, and machine learning, as it allows for the efficient manipulation and understanding of multidimensional data and transformations.


**Eigendecomposition** is a method of expressing a square matrix in terms of its eigenvalues and eigenvectors. For a matrix {% equation_inline A %}, if it can be written as {% equation_inline A = V D V^{-1} %}, where {% equation_inline V %} contains the eigenvectors and {% equation_inline D %} is a diagonal matrix of eigenvalues, then {% equation_inline A %} is said to be diagonalizable. This decomposition reveals the fundamental directions (eigenvectors) and scaling factors (eigenvalues) of the transformation represented by {% equation_inline A %}. Eigendecomposition is useful in simplifying matrix operations and understanding the behavior of linear transformations in geometry processing, physics, and data analysis.

{% equation id="energy" %}
A \in R^{n \times n}
{% endequation %}

For future examples, let’s use the following $3 \times 3$ matrix as $A$:
{% equation %}
A = \begin{pmatrix}
2 & 1 & 0 \\
1 & 2 & 1 \\
0 & 1 & 2
\end{pmatrix}
{% endequation %}

The formula {% equation_inline Av = \lambda v %} means that when you apply the matrix {% equation_inline A %} (a linear transformation) to the vector {% equation_inline v %} (an eigenvector), the result is just the same vector {% equation_inline v %} scaled by the number {% equation_inline \lambda %} (the eigenvalue). 

In other words, {% equation_inline v %} is a special direction for {% equation_inline A %}: instead of changing direction, {% equation_inline A %} only stretches or shrinks {% equation_inline v %} by the factor {% equation_inline \lambda %}. The matrix {% equation_inline A %} does not "store" the eigenvalues; rather, the eigenvalues and eigenvectors are special solutions to this equation for a given {% equation_inline A %}.

<!-- eigenvector: 
    
    Av=λv

    A is a linear transformation matrix
    v is a nonzero vector
    λ is a scalar.

    This equation says: when you apply   -->
<!-- 
v eigenvector of A <=> Av=lambda*v where v not equal zero and v belongs to R^{n} and lambda belongs to R and ||v||=1
Lambda eigenvector of A corresponding to v -->

<!-- Av = lambda v -> Av-lambda v = 0 -> Av -lambda Iv = 0 -> (A-lambdaI)v=0
A-lambdaI = 0 -->

We can rewrite the eigenvector equation step by step:
{% equation %}
Av = \lambda v \\
Av - \lambda v = 0 \\
Av - \lambda I v = 0 \\
(A - \lambda I)v = 0
{% endequation %}

To have a nontrivial solution ({% equation_inline v \neq 0 %}), the matrix {% equation_inline (A - \lambda I) %} must be singular (not invertible). This happens only when its determinant is zero:
{% equation %}
\det(A - \lambda I) = 0
{% endequation %}
This is called the characteristic equation. Solving it gives the eigenvalues {% equation_inline \lambda %} of {% equation_inline A %}. For each eigenvalue, you can then find the corresponding eigenvectors {% equation_inline v %} by solving {% equation_inline (A - \lambda I)v = 0 %}.

<!-- 
A symetric -> lambda' s belongs to R
              v' s belongs to R^{n}
              v' s linearly independent
              v orthogonal (i can get an orthonormal system?)

These are:
Characteristic equation of A
characteristic polynomial of A 
-->

The **characteristic polynomial** of a matrix {% equation_inline A %} is the polynomial you get when you compute {% equation_inline \det(A - \lambda I) %}, where {% equation_inline I %} is the identity matrix and {% equation_inline \lambda %} is a variable. It is called a polynomial because, for an {% equation_inline n \times n %} matrix, expanding the determinant gives a degree {% equation_inline n %} polynomial in {% equation_inline \lambda %}. The roots of this polynomial are the eigenvalues of {% equation_inline A %}. In summary:

{% equation %}
	ext{Characteristic polynomial:}\quad p(\lambda) = \det(A - \lambda I)
{% endequation %}
Solving {% equation_inline p(\lambda) = 0 %} gives all the eigenvalues of {% equation_inline A %}.


#### 1. Subtract {% equation_inline \lambda %} from each diagonal entry to get {% equation_inline A - \lambda I %}: 
  {% equation %}
  A - \lambda I = \begin{pmatrix}
  2 - \lambda & 1 & 0 \\
  1 & 2 - \lambda & 1 \\
  0 & 1 & 2 - \lambda
  \end{pmatrix}
  {% endequation %}

#### 2. Compute the determinant and set it to zero: 
  {% equation %}
  \det(A - \lambda I) = 
  \begin{vmatrix}
  2 - \lambda & 1 & 0 \\
  1 & 2 - \lambda & 1 \\
  0 & 1 & 2 - \lambda
  \end{vmatrix}
  = 0
  {% endequation %}

#### 3. Expand the determinant: 
   {% equation %}
  (2-\lambda)\left[(2-\lambda)(2-\lambda) - 1\cdot1\right] - 1\left[1(2-\lambda) - 1\cdot0\right]
  {% endequation %}
  which simplifies to:
  {% equation %}
  (2-\lambda)\left[(2-\lambda)^2 - 1\right] - (2-\lambda)
  {% endequation %}
  {% equation %}
  = (2-\lambda)\left[(2-\lambda)^2 - 1\right] - (2-\lambda)
  {% endequation %}
  {% equation %}
  = (2-\lambda)\left[(2-\lambda)^2 - 1 - 1\right]
  {% endequation %}
  {% equation %}
  = (2-\lambda)\left[(2-\lambda)^2 - 2\right]
  {% endequation %}


#### 4. Set this equal to zero and solve for {% equation_inline \lambda %}:
  {% equation %}
  (2-\lambda)\left[(2-\lambda)^2 - 2\right] = 0
  {% endequation %}

So the eigenvalues are the solutions to {% equation_inline 2-\lambda = 0 %} and {% equation_inline (2-\lambda)^2 - 2 = 0 %}.

- {% equation_inline 2-\lambda = 0 \implies \lambda_1 = 2 %}
- {% equation_inline (2-\lambda)^2 - 2 = 0 \implies (2-\lambda)^2 = 2 \implies 2-\lambda = \pm\sqrt{2} \implies \lambda_2 = 2+\sqrt{2},\ \lambda_3 = 2-\sqrt{2} %}

So the eigenvalues are {% equation_inline \boxed{2,\ 2+\sqrt{2},\ 2-\sqrt{2}} %}.


### Obtain Eigenvectors
Let’s work this one out step by step. You’ve already correctly found the eigenvalues of

{% equation %}
A = \begin{bmatrix}
2 & 1 & 0 \\
1 & 2 & 1 \\
0 & 1 & 2
\end{bmatrix}
{% endequation %}

The eigenvalues are:

{% equation %}
\lambda_1 = 2, \quad \lambda_2 = 2+\sqrt{2}, \quad \lambda_3 = 2-\sqrt{2}.
{% endequation %}

Now let’s find the eigenvectors.

---

#### 1. General method

For each eigenvalue {% equation_inline \lambda %}, solve

{% equation %}
(A - \lambda I) v = 0
{% endequation %}

for {% equation_inline v \neq 0 %}.

---

#### 2. Eigenvector for {% equation_inline \lambda_1 = 2 %}

{% equation %}
A - 2I =
\begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 0
\end{bmatrix}
{% endequation %}

So we need to solve:

{% equation %}
\begin{cases}
y = 0 \\
x+z = 0 \\
y = 0
\end{cases}
{% endequation %}

That gives {% equation_inline y=0 %}, {% equation_inline z=-x %}.
So an eigenvector is

{% equation %}
v_1 = \begin{bmatrix} 1 \\ 0 \\ -1 \end{bmatrix}
{% endequation %}

---

#### 3. Eigenvector for {% equation_inline \lambda_2 = 2+\sqrt{2} %}

{% equation %}
A - (2+\sqrt{2})I =
\begin{bmatrix}
-\sqrt{2} & 1 & 0 \\
1 & -\sqrt{2} & 1 \\
0 & 1 & -\sqrt{2}
\end{bmatrix}
{% endequation %}

System:

{% equation %}
\begin{cases}
-\sqrt{2}x + y = 0 \quad \Rightarrow \; y = \sqrt{2} x \\
x - \sqrt{2} y + z = 0 \\
y - \sqrt{2} z = 0
\end{cases}
{% endequation %}

From the first: {% equation_inline y = \sqrt{2}x %}.
From the third: {% equation_inline z = y / \sqrt{2} = x %}.
So eigenvector is

{% equation %}
v_2 = \begin{bmatrix} 1 \\ \sqrt{2} \\ 1 \end{bmatrix}
{% endequation %}

---

#### 4. Eigenvector for {% equation_inline \lambda_3 = 2-\sqrt{2} %}

{% equation %}
A - (2-\sqrt{2})I =
\begin{bmatrix}
\sqrt{2} & 1 & 0 \\
1 & \sqrt{2} & 1 \\
0 & 1 & \sqrt{2}
\end{bmatrix}
{% endequation %}

System:

{% equation %}
\begin{cases}
\sqrt{2}x + y = 0 \quad \Rightarrow \; y = -\sqrt{2}x \\
x + \sqrt{2}y + z = 0 \\
y + \sqrt{2} z = 0
\end{cases}
{% endequation %}

From the first: {% equation_inline y = -\sqrt{2}x %}.
From the third: {% equation_inline z = -y/\sqrt{2} = x %}
So eigenvector is

{% equation %}
v_3 = \begin{bmatrix} 1 \\ -\sqrt{2} \\ 1 \end{bmatrix}
{% endequation %}

---

#### 5. Final result

The eigenpairs are:

* $\lambda_1 = 2 \;\;\Rightarrow v_1 = [1,0,-1]^T$
* $\lambda_2 = 2+\sqrt{2} \;\;\Rightarrow v_2 = [1,\sqrt{2},1]^T$
* $\lambda_3 = 2-\sqrt{2} \;\;\Rightarrow v_3 = [1,-\sqrt{2},1]^T$

---

✅ Notice: since $A$ is **symmetric**, the eigenvectors are mutually orthogonal (check dot products — they’re zero).

---















<!-- Av_i =lambda_i v_i
A*V=V*D, V=[[...],[v1,v2,...,vn],[...]].T
A*[v1,v2,...,vn]=[col.Av_1 col.Av_2 ... colAv_n]
V*V.T = I = V.T*V
V*D = [col.lambda_1*v_1 ... col.lambda_n*v_n]
A=V*V.T=V*D*V.T
A=V*D*V.T

example:
with Av=λv and |A-λI|=0
A = (3 1)
    (1 3)

Solve λ, given the values of λ solve (A-λ_1*I)*v_1=0

what happens when A is (1 4)
                       (3 2) -->
## Fitting a line (least squares)

Suppose you have data points {% equation_inline (x_i, y_i) %} and want to fit a line {% equation_inline y = a x + b %} by minimizing the squared error:

{% equation %}
E(a, b) = \sum_{i=1}^{n} (y_i - a x_i - b)^2
{% endequation %}

To find the best-fit parameters {% equation_inline a %} and {% equation_inline b %}, take derivatives of {% equation_inline E(a, b) %} with respect to {% equation_inline a %} and {% equation_inline b %}, set them to zero, and solve the resulting system:

{% equation %}
\frac{\partial E}{\partial a} = 0, \quad \frac{\partial E}{\partial b} = 0
{% endequation %}

This yields two linear equations for {% equation_inline a %} and {% equation_inline b %}.

Alternatively, in matrix form, for general least squares problems:

{% equation %}
E(x) = \|A x - b\|^2 = (A x - b)^T (A x - b)
{% endequation %}

Expanding:

{% equation %}
E(x) = x^T A^T A x - 2 x^T A^T b + b^T b
{% endequation %}

Setting the gradient to zero:

{% equation %}
\nabla E = 2 A^T A x - 2 A^T b = 0
{% endequation %}

So the least squares solution satisfies:

{% equation %}
A^T A x = A^T b
{% endequation %}

This is the normal equation for least squares.


## Analysis (curvatures)
## Simplification

- Lagrange multiplier & lagrange functions

  Lagrange multipliers and Lagrange functions are mathematical tools used to find the maximum or minimum of a function subject to one or more constraints. Instead of searching for extrema in the whole space, they allow us to optimize a function while ensuring that certain conditions (constraints) are always satisfied. The method introduces new variables (the multipliers) and combines the original function and the constraints into a single function, called the Lagrangian. By finding the critical points of this Lagrangian, we can solve constrained optimization problems efficiently.

  To simplify the process we can eve use matrix notation. You want to minimize a quadratic error function {% equation_inline E(n,d) %} with n = normal vector and d=distance from the origin of an hyperplane.

{% equation %}
  E(n,d)
{% endequation %}

  * n = vector to optimize (normal to plane).
  * d = scalar shift.

Given points 
{% equation_inline p_i %}, the signed distance from a point to the plane is:
{% equation %}
  dist(p_i,n,d) = n \cdot p_i + d
{% endequation %}
If we want the best-fit plane (least squares), we minimize:
{% equation %}
  E(n,d) = \sum{(n \cdot p_i + d)^2}
{% endequation %}
That’s the quadratic error function.

If you try to minimize E(n,d) directly, the trivial solution is n=0,d=0, which is meaningless.

We need a constraint:
{% equation %}
  ||n||^2=1
{% endequation %}
to ensure the normal is a unit vector.

We now minimize:
{% equation %}
L(n,d,λ)=E(n,d)−λ(∥n∥^2−1)
{% endequation %}
* Here, λ is the Lagrange multiplier.
* Taking derivatives w.r.t. n,d,λ gives a system of equations.
* Solving it gives the optimal plane normal n and offset d.


The derivatives are:

* With respect to {% equation_inline n %}:
{% equation %}
\frac{\partial L}{\partial n} = 2 C n + 2 d c - 2 \lambda n
{% endequation %}
  (where {% equation_inline C %} and {% equation_inline c %} are the covariance matrix and mean vector, depending on how {% equation_inline E(n,d) %} is written)

* With respect to {% equation_inline d %}:
{% equation %}
\frac{\partial L}{\partial d} = 2 c^T n + 2 d N
{% endequation %}
  (where {% equation_inline N %} is the number of points)

* With respect to {% equation_inline \lambda %}:
{% equation %}
\frac{\partial L}{\partial \lambda} = - (\|n\|^2 - 1)
{% endequation %}

Setting these derivatives to zero gives the system of equations whose solution yields the optimal {% equation_inline n %} and {% equation_inline d %}.

<!-- 
the professor said "solutions to opt f(x) s.t g(x)=0
are the critical points of Lagrange"

It is doing partial derivative of lambda, x and y (not sure why)
We are given a function for a line 

l = ax + by + c = 0
but also an alternative version which can  also represent spheres. This is a bit confusing
h^t p + d = 0
||n|| = 1
d(p,l)=n^T p + d
E(l) = sum(n, i=1) d(p_i, l)^2 = sum (n^T p_1 + d)^2
We also can understand Error as 
E(n,d); OPT E(n,d) s.t. ||n||=1

We end up with n^T C n - lambda(n^T n -1) where C is a covariance matrix

This means 
derivative lagrange / derivative n = 2*C*n-2*lambda*n=0 which means C*n = lanbda*n
n will become Eigenvectors of C
lambda will become eigenvalues of C

E(n,d) = n^T * C * n = n^T * lambda * n = lambda*n^T*n = lambda

n eigenv of C with smallest eigenv lambda -->

## getting points
Having multiple point clouds knowing the camera position allows us to know the geometry of the photographed object

### Alignment (registration)

Given two point clouds, {% equation_inline P = \{P_i\} \text{ for } 1 \leq i \leq n %} and {% equation_inline Q = \{Q_i\} \text{ for } 1 \leq i \leq n %}, we seek a transformation—typically a rotation and translation—that aligns {% equation_inline Q %} to {% equation_inline P %}.

The goal is to find a rotation matrix {% equation_inline R %} and a translation vector {% equation_inline t %} such that:

{% equation %}
P_i \approx R Q_i + t \quad \text{for } 1 \leq i \leq n
{% endequation %}

This process is known as point cloud registration or alignment.

### SVD (singular value decomposition)
{% equation_inline A \in \mathbb{R}^{n \times n} %}
Eigendecomposition:
{% equation %}
A = V D V^T
{% endequation %}
where
* {% equation_inline V %} contains the eigenvectors of {% equation_inline A %},
* {% equation_inline D %} is a diagonal matrix of eigenvalues,
* {% equation_inline V^T %} is the transpose of {% equation_inline V %}.

Singular Value Decomposition (SVD):
{% equation_inline A \in \mathbb{R}^{m \times n} %}
{% equation %}
A = U \Sigma V^T
{% endequation %}
where
* {% equation_inline U \in \mathbb{R}^{m \times m} %} is an orthogonal matrix,
* {% equation_inline V \in \mathbb{R}^{n \times n} %} is an orthogonal matrix,
* {% equation_inline \Sigma \in \mathbb{R}^{m \times n} %} is a diagonal matrix with singular values.

Orthogonal matrices ({% equation_inline U %} and {% equation_inline V %}) represent rotations or reflections.


{% equation %}
\Sigma = \operatorname{diag}(\alpha_1, \alpha_2, \ldots, \alpha_{\min(m,n)})
{% endequation %}
where each diagonal entry {% equation_inline \alpha_i %} is a singular value of {% equation_inline A %}, and {% equation_inline \alpha_i \geq 0 %}.


Given a linear system {% equation_inline Ax = b %}, where {% equation_inline A \in \mathbb{R}^{r \times n} %}, we can use the Singular Value Decomposition (SVD) to solve for {% equation_inline x %}. If {% equation_inline A = U \Sigma V^T %}, then:

{% equation %}
U \Sigma V^T x = b
{% endequation %}

Multiplying both sides by {% equation_inline U^T %}:

{% equation %}
\Sigma V^T x = U^T b
{% endequation %}

Let {% equation_inline y = V^T x %}, so:

{% equation %}
\Sigma y = U^T b
{% endequation %}

Solving for {% equation_inline y %}:

{% equation %}
y = \Sigma^{-1} U^T b
{% endequation %}

Then, recovering {% equation_inline x %}:

{% equation %}
x = V y = V \Sigma^{-1} U^T b
{% endequation %}

This expresses the solution to {% equation_inline Ax = b %} using SVD.


### Simpler Alignment
Given two point clouds:
- {% equation_inline P = \{P_i\} \text{ for } 1 \leq i \leq n %}
- {% equation_inline Q = \{q_i\} \text{ for } 1 \leq i \leq n %}

Assume:
- Both sets have the same number of points ({% equation_inline n %}).
- Each {% equation_inline P_i %} corresponds to {% equation_inline q_i %}.

We seek a rotation matrix {% equation_inline R \in \mathbb{R}^{3 \times 3} %} and a translation vector {% equation_inline t \in \mathbb{R}^3 %} such that:

{% equation %}
P_i \approx R q_i + t
{% endequation %}

The alignment error function is:

{% equation %}
E(R, t) = \sum_{i=1}^{n} \| P_i - R q_i - t \|^2
{% endequation %}

To solve for {% equation_inline t %}:
- Compute the centroids of {% equation_inline P %} and {% equation_inline Q %}:

{% equation %}
P' = \frac{1}{n} \sum_{i=1}^{n} P_i
{% endequation %}
{% equation %}
q' = \frac{1}{n} \sum_{i=1}^{n} q_i
{% endequation %}

- The optimal translation is:

{% equation %}
t = P' - R q'
{% endequation %}

So, the aligned points satisfy:

{% equation %}
P' = R q' + t
{% endequation %}

### Steps
1. Compute centroids {% equation_inline \{p', q'\} %}
2. Compute centroids adjusted:
    {% equation_inline p'_i = p_i - \bar{p} %}, {% equation_inline q'_i = q_i - \bar{q} %}

3. {% equation_inline S = Q'P'^T = (q'_1, q'_2, \ldots, q'_n) \cdot ([p'_1], [p'_2], \ldots, [p'_n]) %}
4. {% equation_inline S = U \Sigma V^T %}
5. {% equation_inline R = V U^T %}
6. {% equation_inline t = \bar{p} - R \bar{q} %}

### Iterative closest point (ICP)
1. Compute correspondence
   - For every {% equation_inline q_i %} find closest {% equation_inline p_k %}
2. Use simpler alignment (SVD)
   * Discard points in {% equation_inline P %} without correspondence
   * Replicate points in {% equation_inline P %} multiply selected
   - From these points we get a {% equation_inline P' %} that will be used temporarily
   - Get {% equation_inline R, t %}
3. Apply transformation to {% equation_inline Q %}: {% equation_inline q'_i \leftarrow R q_i + t %}
4. Repeat from step 1
  

### Stop criteria
1. {% equation_inline \|R-I\|_F < \epsilon %} where {% equation_inline \|t\| < \epsilon %}
   1. Correspondence constant
2. Maximum number of iterations
   1. Need initial rough alignment (there are better alternatives in the literature)

### Compute border point status
One of the possible problems is that point clouds are apart and the closest point for each other is a single point from {% equation_inline P %} for all {% equation_inline Q %} points.

One solution could be to remove all overlapping points. The single point in {% equation_inline P %} that corresponds to multiple {% equation_inline Q %} points is called a border point on {% equation_inline P %}.

**How do we detect border points and overlapped points?**

We will recognize border points by checking the size of the angle that does not possess any point.

For every point {% equation_inline P_i %}:
  1. Compute K-NN (Nearest Neighbours) of {% equation_inline P_i %}: {% equation_inline \{P_{ij}\}_{j=1}^k %}
  2. PCA → Eigenvectors {% equation_inline (v_1, v_2, v_3) %} (where {% equation_inline v_3 %} is normal) + {% equation_inline P_i %} = local frame (local coordinate system)
  3. Compute local coordinates: {% equation_inline (x^\prime_{ij}, y^\prime_{ij}, z^\prime_{ij}) = ((P_{ij} - P_i) \cdot v_1, (P_{ij} - P_i) \cdot v_2, (P_{ij} - P_i) \cdot v_3) %}
  4. Discard {% equation_inline z^\prime_{ij} %}
  5. Compute direction by taking the angle: {% equation_inline \alpha_{ij} = \text{atan2}(y^\prime_{ij}, x^\prime_{ij}) %}
  6. Sort {% equation_inline \alpha_{ij} %}
  7. Compute differences of adjacent pairs:
     - We call them {% equation_inline \Delta\alpha_{ij} %}
  8. {% equation_inline \max(\Delta\alpha_{ij}) \geq \beta \iff P_i %} is a border point

**Parameters:** {% equation_inline K, \beta %}

### Multiple Scans
Overlap → 1st step of ICP → compute correspondence

Overlap = Percentage of discarded points of {% equation_inline Q %}

1. Select 2 unaligned clouds
2. Perform ICP on them, result: (successful \| fail)
   1. Successful: {% equation_inline \|R-I\|_F < \epsilon %} and {% equation_inline \|t\| \approx 0 %} → fuse the 2 clouds
   2. Fail → try different initialization

### Solving linear systems
{% equation_inline Ax = b %}, {% equation_inline A \in \mathbb{R}^{n \times n} %}, {% equation_inline x, b \in \mathbb{R}^n %}

Apply solvers:
* **Direct**: Performs operations to solve exactly (except machine precision)
* **Iterative**: Start with {% equation_inline x_0 %} estimation and then refine estimation

### Properties and requirements to how we want to resolve the system
* Required performance
* Required accuracy
* Limits for storage
* Problem dimension: {% equation_inline n %} (where {% equation_inline A \in \mathbb{R}^{n \times n} %}), {% equation_inline n > 10000 %} → LARGE
* Sparsity of {% equation_inline A %} = Percentage of zeros in {% equation_inline A %} (sparse \| dense)
* Symmetry of {% equation_inline A %}
* Positive definiteness of {% equation_inline A %}:
  * {% equation_inline A %} is invertible ({% equation_inline \det(A) \neq 0 %}) and {% equation_inline x^T A x > 0 %} for all {% equation_inline x \neq 0 %} {% equation_inline \iff A %} is positive definite

### Decompositions

**Direct methods:**

* **LU decomposition**: {% equation_inline A = LU %}
  * {% equation_inline L %} = lower triangular
  * {% equation_inline U %} = upper triangular
  * {% equation_inline Ax = b \implies LUx = b \implies Lz = b, Ux = z %}
  
* **QR decomposition**: {% equation_inline A = QR %}
  * {% equation_inline R %} = upper triangular
  * {% equation_inline Q %} = orthogonal
  * {% equation_inline Ax = b \implies QRx = b \iff Rx = z %}
  * {% equation_inline z = Q^T b %} (i.e., {% equation_inline Qz = b %})
  
* **Cholesky decomposition**:
  * {% equation_inline A %} is symmetric and positive definite
  * {% equation_inline A = LL^T %} where {% equation_inline L %} is lower triangular

**Iterative methods:**

* **Conjugate Gradient (CG)**: {% equation_inline A %} symmetric and positive definite
* **Biconjugate Gradient (Bi-CG)**: For non-symmetric systems 


<!-- ## Smoothing (remove noise)
## Remeshing
## Parametrization
## Model repair
## Editing and reformation
## Synthesis
## Symetry detection -->

# Session 3
Many reconstruction algorithms reduce (or contain subproblems that reduce) to linear systems. Choosing the right solver is crucial for performance and memory.

in general is more complex than this, but this works for most problems to choose a solver
1. Decide the size of the system. Depends a little bit on the system. size is  a convention made by people, not a mathematical solution
   1. large: means matrix is sparse. 
      1. A is symetric and positive matrix, then we can use CG
      2. if not, Bi-CG
      3. if it is large and dense: 
         1. Transform into sparse
         2. Specific solver for the problem we are going to solve.
         3. Brute force approach: highly parallel solver + hardware
   2. small: A symm+pos.det->cholesky
      1. other options QR & LU decomp.

## reconstruction
Reconstruction

**Input:** We are given a set of sample points {% equation_inline \{p_i \in \mathbb{R}^3\}_{1 \leq i \leq n} %} together with their normal vectors {% equation_inline \{n_i \in \mathbb{R}^3\}_{1 \leq i \leq n} %}. Our goal is to use a reconstruction algorithm to produce a surface representation (typically a triangle mesh).

**Sampling density:** The set {% equation_inline Q = \{q_i\} %} is called {% equation_inline \epsilon %}-dense with respect to the surface {% equation_inline S %} if every point on the surface has at least one sample point within distance {% equation_inline \epsilon %}. Formally, for all {% equation_inline q \in S %}, there exists some {% equation_inline p_i %} such that {% equation_inline q \in \text{Sphere}(p_i, \epsilon) %}.

**Relationship between samples and surface:** Each input point {% equation_inline p_i %} can be expressed as {% equation_inline p_i = q_i + e_i %}, where {% equation_inline q_i \in S %} is the closest point on the true surface, and {% equation_inline e_i %} is the noise vector.

**Noise level:** When the magnitude of the noise is bounded by {% equation_inline \|e_i\| < \delta %}, we say that {% equation_inline P %} is {% equation_inline \delta %}-noisy.

**Combined property:** If our point cloud {% equation_inline P %} is {% equation_inline \delta %}-noisy with respect to a set {% equation_inline Q %} that lies on the surface {% equation_inline S %}, and {% equation_inline Q %} itself is {% equation_inline \epsilon %}-dense with respect to {% equation_inline S %}, then we conclude that {% equation_inline P %} is both {% equation_inline \epsilon %}-dense and {% equation_inline \delta %}-noisy with respect to the surface {% equation_inline S %}.

**Standard reconstruction pipeline:**
  - Start with a point cloud
  - Reconstruct an implicit function {% equation_inline f: D \to \mathbb{R} %} where {% equation_inline D \subset \mathbb{R}^3 %}
  - Extract the zero-level isosurface to obtain a triangle mesh
  
  The implicit function {% equation_inline f %} assigns a signed distance value to each point, where:
    - {% equation_inline f(p) = 0 %} means the point {% equation_inline p %} lies exactly on the surface {% equation_inline S %}
    - {% equation_inline f(p) > 0 %} means the point {% equation_inline p %} is outside the surface {% equation_inline S %}
    - {% equation_inline f(p) < 0 %} means the point {% equation_inline p %} is inside the surface {% equation_inline S %}

## algorithms for reconstructions

Reconstruction algorithms can be classified into two main categories:

- **Interpolatory:** For all {% equation_inline i %} where {% equation_inline 1 \leq i \leq n %}, we have {% equation_inline f(p_i) = 0 %}. The reconstructed surface passes exactly through all sample points.

- **Approximative:** For all {% equation_inline i %} where {% equation_inline 1 \leq i \leq n %}, we have {% equation_inline f(p_i) \approx 0 %}. The reconstructed surface passes close to, but not necessarily through, the sample points.

### Hoppe et al. Method

**Properties:** This is NOT a smooth function. It is an interpolatory method.

**Algorithm:** For a query point {% equation_inline p %}:

{% equation %}
\begin{aligned}
i &\leftarrow \text{Index of } p_i \text{ closest to } p \\
z &\leftarrow p - ((p - p_i) \cdot n_i) n_i \\
f(p) &\leftarrow \begin{cases}
(p - p_i) \cdot n_i & \text{if } \|z - p_i\| \leq \delta \\
\text{undefined} & \text{otherwise}
\end{cases}
\end{aligned}
{% endequation %}

**Problems:**
  - Having {% equation_inline f(p) %} undefined in some regions means we get holes in the reconstruction
  - Discontinuities arise because this is an interpolatory method, which is usually not desired

### Moving Least Squares (MLS)

**Properties:** This is a smooth function. Approximative algorithm. No holes, but no quality guarantees for empty space.

**Notation:**
- {% equation_inline p_i %} are the input sample points from our point cloud ({% equation_inline i = 1, \ldots, n %})
- {% equation_inline n_i %} is the normal vector at sample point {% equation_inline p_i %}
- {% equation_inline p %} is an arbitrary query point where we want to evaluate the implicit function

**Algorithm:** We define local bump functions:

{% equation %}
f_i(p) = (p - p_i) \cdot n_i
{% endequation %}

The final implicit function is a weighted average:

{% equation %}
f(p) = \frac{\sum_{i=1}^{n} W_i(p) f_i(p)}{\sum_{i=1}^{n} W_i(p)}
{% endequation %}

**Weight function:** For the weight function {% equation_inline W_i(p) %} we can choose various forms. A common choice is:

{% equation %}
W_i(p) = \frac{\exp\left(-\frac{\|p - p_i\|^2}{\sigma^2}\right)}{A_i}
{% endequation %}

where {% equation_inline A_i %} is the number of points closer to {% equation_inline p %} than {% equation_inline \sigma %}.

**Requirement:** This requires {% equation_inline p %} to be {% equation_inline \sigma %}-dense.

### Radial Basis Functions (RBF)

**Idea:** We incorporate prior information (usually a smoothness prior) by expressing the implicit function as a weighted sum of basis functions.

The summation of {% equation_inline n %} bump functions passes through some points but not necessarily through others:

{% equation %}
f(p) = \sum_{i=1}^{n} c_i f_i(p)
{% endequation %}

**Basis functions:** We define:

{% equation %}
f_i(p) = \phi(\|p - p_i\|)
{% endequation %}

A common choice for {% equation_inline \phi %} is the Gaussian:

{% equation %}
\phi(r) = \exp\left(-\frac{r^2}{2\sigma^2}\right)
{% endequation %}

**Solving for coefficients:** Given data pairs {% equation_inline (p_i, v_i) %} for {% equation_inline 1 \leq i \leq n %}, we want {% equation_inline f(p_i) = v_i %}. This gives us:

{% equation %}
f(p_i) = v_i = \sum_{j=1}^{n} c_j \phi(\|p_i - p_j\|)
{% endequation %}

This results in a linear system with {% equation_inline n %} equations and {% equation_inline n %} unknowns (the coefficients {% equation_inline c_i %}):

{% equation %}
A \mathbf{c} = \mathbf{v}
{% endequation %}

where the matrix {% equation_inline A %} has entries {% equation_inline A_{ij} = \phi(\|p_i - p_j\|) %}.

**Problems and Solutions:**

1. **Trivial solution:** If we set all {% equation_inline v_i = 0 %}, then {% equation_inline \mathbf{c} = \mathbf{0} %} is a solution (not useful).
   
   **Solution:** Add synthetic points. For every point {% equation_inline p_i %}, generate:
   - {% equation_inline p_i^+ = p_i + d \cdot n_i %} with {% equation_inline f(p_i^+) = d %}
   - {% equation_inline p_i^- = p_i - d \cdot n_i %} with {% equation_inline f(p_i^-) = -d %}
   
   **New problem:** This triples the point set size (factor of 3), making the system much larger.

2. **Numerical instability:** As {% equation_inline n %} increases, {% equation_inline A\mathbf{c} = \mathbf{v} %} becomes ill-conditioned.
   
   **Solution - Regularization:** Use {% equation_inline A' = A + \lambda I %} instead:
   
{% equation %}
(A + \lambda I) \mathbf{c} = \mathbf{v}
{% endequation %}
   
   This changes the method from interpolatory to approximative.

3. **Large point sets:** For very large point clouds, triplicating the size is computationally prohibitive.
   
   **Solution - Compact support:** Use a basis function with compact support:
   
{% equation %}
\phi(r) = \begin{cases}
\exp\left(-\frac{r^2}{2\sigma^2}\right) & \text{if } r < 3\sigma \\
0 & \text{otherwise}
\end{cases}
{% endequation %}
   
   This makes the matrix {% equation_inline A %} sparse, significantly reducing computational cost.

### Poisson Surface Reconstruction

**Idea:** Given a set of oriented points (positions and normals), this method solves for an indicator function that is 1 inside the surface and 0 outside.

### SSD (Screened Poisson Surface Reconstruction)



# Session 4: Curvature Analysis

Having completed the basic reconstruction pipeline and obtained normals and triangles, we now turn our attention to computing **curvature**, which we denote as {% equation_inline K %}. Curvature measures how much a surface bends at any given point.

## Introduction to Curvature

Curvature has several interesting properties that make it a fundamental concept in geometry processing:

- **Translation and rotation invariant:** The curvature of a surface doesn't change when we move or rotate it
- **Local surface characteristic:** It provides a geometric signature based purely on local surface shape
  
### Historical Definitions of Curvature

Several equivalent definitions of curvature have been developed throughout mathematical history:

**Osculating Circle Approach**

The classical definition uses the concept of an **osculating circle** (from the Latin "oscular," meaning "to kiss"). The osculating circle is the circle that best approximates a curve at a given point. If this circle has radius {% equation_inline R %}, the curvature is defined as:

{% equation %}
K = \frac{1}{R}
{% endequation %}

The center of the osculating circle can be found as the intersection of perpendiculars to the curve.

**Newton's Observation**

Newton discovered a connection between the second derivative of a function and its curvature. For a curve described by {% equation_inline y = f(x) %} at a point where {% equation_inline f'(0) = 0 %}, the curvature at {% equation_inline (0,0) %} equals {% equation_inline f''(0) %}.

**Rate of Turning Definition**

Later mathematicians realized that curvature can be understood as the rate of turning of the tangent vector:

{% equation %}
K = \frac{d\gamma}{dS}
{% endequation %}

where {% equation_inline \gamma %} is the angle of the tangent vector and {% equation_inline S %} is arc length.

## Curvature on Surfaces

While the above definitions work well for curves, we need to extend the concept to surfaces.

### Principal Curvatures

For a surface {% equation_inline S %} at a point {% equation_inline p %}, we can consider the curvature along different directions. If we intersect the surface with a plane {% equation_inline \pi %} containing the surface normal at {% equation_inline p %}, we obtain a curve with curvature {% equation_inline K_{p,\pi} %}.

For a smooth surface {% equation_inline S %} (with continuous second derivatives), we can define:

- {% equation_inline K_{\min} %} and {% equation_inline K_{\max} %}: The minimum and maximum curvatures across all such planes
- {% equation_inline v_{\min} %} and {% equation_inline v_{\max} %}: The directions (in the tangent plane) corresponding to these extremal curvatures

These are called the **principal curvatures** and **principal directions** of {% equation_inline S %} at {% equation_inline p %}.

**Example:** For a cylinder with radius {% equation_inline R %}:
- {% equation_inline K_{\min} = 0 %} (along the axis direction - no bending)
- {% equation_inline K_{\max} = \frac{1}{R} %} (in the circular direction)

### Surface Classification by Curvature

The sign and magnitude of the principal curvatures tell us about the local shape:

- **Convex:** {% equation_inline K_{\min} > 0 %}, {% equation_inline K_{\max} > 0 %} (both principal curvatures positive, like a sphere)
- **Concave:** {% equation_inline K_{\min} < 0 %}, {% equation_inline K_{\max} < 0 %} (both negative, like the inside of a bowl)
- **Parabolic convex:** {% equation_inline K_{\min} = 0 %}, {% equation_inline K_{\max} > 0 %} (flat in one direction, curved in the other)
- **Parabolic concave:** {% equation_inline K_{\min} < 0 %}, {% equation_inline K_{\max} = 0 %}
- **Saddle:** {% equation_inline K_{\min} < 0 %}, {% equation_inline K_{\max} > 0 %} (curved up in one direction, down in another, like a horse saddle)

**Sign Convention Example:**

For a parabola {% equation_inline y = ax^2 %}:
- Second derivative: {% equation_inline f''(x) = 2a %}
- If {% equation_inline a > 0 %} and normal points in {% equation_inline -y %} direction: convex
- For {% equation_inline y = -ax^2 %} with {% equation_inline a > 0 %}: {% equation_inline f''(x) = -2a < 0 %}, this is concave

## Computing Curvature on Discrete Surfaces

### Curvature in 2D: A Motivating Example

In 2D, for a curve given as a set of points, we can compute curvature using the following approach:

1. **Transform to local coordinates:** Move the curve so it crosses the origin {% equation_inline (0,0) %} at a point where the first derivative is also zero
2. **Fit a quadratic:** Using least squares, fit {% equation_inline f(x) = ax^2 + bx + c %} to a neighborhood of points
3. **Extract curvature:** Compute {% equation_inline K = f''(0) = 2a %}

### Extension to Surfaces

For triangle meshes, we extend this idea to surfaces. We work with local neighborhoods of vertices, often restricting neighbors along the mesh geometry (the 1-ring or K-nearest neighbors).

**Steps:**

1. **Transform to local frame** (analogous to centering in 2D)
2. **Fit a quadratic surface**
3. **Compute the surface's "second derivative"**

### The Algorithm

**Input:**
- Point {% equation_inline p_i %}
- {% equation_inline K %}-nearest neighbors {% equation_inline \{p_{ij}\}_{1 \leq j \leq K} %}

**Output:**
- Principal curvatures: {% equation_inline K_{\min}(p_i) %}, {% equation_inline K_{\max}(p_i) %}
- Principal directions: {% equation_inline v_{\min}(p_i) %}, {% equation_inline v_{\max}(p_i) %}

**Step 1: Construct Local Frame**

We build an orthonormal coordinate system at {% equation_inline p_i %}:

{% equation %}
\begin{aligned}
V &= -n_i \quad \text{(negative normal direction)} \\
U &= \text{OX} \times V \quad \text{(if } V \parallel \text{OX, use OY or OZ instead)} \\
W &= U \times V \quad \text{(complete the right-handed frame)}
\end{aligned}
{% endequation %}

**Step 2: Express Neighbors in Local Coordinates**

For each neighbor {% equation_inline p_{ij} %}, compute its coordinates in the local frame:

{% equation %}
\begin{aligned}
u_{ij} &= (p_{ij} - p_i) \cdot U \\
v_{ij} &= (p_{ij} - p_i) \cdot V \\
w_{ij} &= (p_{ij} - p_i) \cdot W
\end{aligned}
{% endequation %}

**Step 3: Fit Quadratic Surface**

We fit a quadratic surface of the form:

{% equation %}
w(u, v) = au^2 + buv + cv^2 + du + ev + f
{% endequation %}

This can be written as {% equation_inline w = \mathbf{q}^T \mathbf{s} %} where:

{% equation %}
\mathbf{q} = \begin{pmatrix} u^2 \\ uv \\ v^2 \\ u \\ v \\ 1 \end{pmatrix}, \quad
\mathbf{s} = \begin{pmatrix} a \\ b \\ c \\ d \\ e \\ f \end{pmatrix}
{% endequation %}

We minimize the least squares error:

{% equation %}
E(\mathbf{s}) = \sum_{j=1}^{K} (w(u_{ij}, v_{ij}) - w_{ij})^2 = \sum_{j=1}^{K} (\mathbf{q}_{ij}^T \mathbf{s} - w_{ij})^2
{% endequation %}

Expanding:

{% equation %}
E(\mathbf{s}) = \mathbf{s}^T \left(\sum_{j=1}^{K} \mathbf{q}_{ij} \mathbf{q}_{ij}^T\right) \mathbf{s} - 2\mathbf{s}^T \left(\sum_{j=1}^{K} w_{ij} \mathbf{q}_{ij}\right) + \sum_{j=1}^{K} w_{ij}^2
{% endequation %}

Setting {% equation_inline \nabla E = 0 %} gives the normal equations:

{% equation %}
\left(\sum_{j=1}^{K} \mathbf{q}_{ij} \mathbf{q}_{ij}^T\right) \mathbf{s} = \sum_{j=1}^{K} w_{ij} \mathbf{q}_{ij}
{% endequation %}

Or more compactly: {% equation_inline A\mathbf{s} = \mathbf{b} %} where {% equation_inline A \in \mathbb{R}^{6 \times 6} %} and {% equation_inline \mathbf{b} \in \mathbb{R}^6 %}.

**Step 4: Compute the Hessian**

The partial derivatives of {% equation_inline w(u,v) %} are:

{% equation %}
\begin{aligned}
\frac{\partial w}{\partial u} &= 2au + bv + d \\
\frac{\partial^2 w}{\partial u^2} &= 2a \\
\frac{\partial^2 w}{\partial u \partial v} &= b \\
\frac{\partial^2 w}{\partial v^2} &= 2c
\end{aligned}
{% endequation %}

The Hessian matrix is:

{% equation %}
H_w = \begin{pmatrix}
2a & b \\
b & 2c
\end{pmatrix}
{% endequation %}

**Step 5: Eigen-decomposition**

Perform eigen-decomposition of {% equation_inline H_w %}:

{% equation %}
H_w = \lambda_1 v_1 v_1^T + \lambda_2 v_2 v_2^T
{% endequation %}

where:
- {% equation_inline \lambda_1 = K_{\min} %}, {% equation_inline \lambda_2 = K_{\max} %} (the principal curvatures)
- {% equation_inline v_1 = v_{\min} %}, {% equation_inline v_2 = v_{\max} %} (the principal directions)

## Derived Curvature Measures

### Gaussian Curvature

The **Gaussian curvature** is the product of principal curvatures:

{% equation %}
K_G = K_{\min} \cdot K_{\max}
{% endequation %}

### Mean Curvature

The **mean curvature** is the average of principal curvatures:

{% equation %}
H = \frac{K_{\min} + K_{\max}}{2}
{% endequation %}

### Surface Classification Using Gaussian and Mean Curvature

The Gaussian and mean curvatures provide an alternative classification scheme:

**When {% equation_inline K_G = 0 %}** (one principal curvature is zero):
- {% equation_inline H = 0 %}: **Planar**
- {% equation_inline H < 0 %}: **Parabolic concave**
- {% equation_inline H > 0 %}: **Parabolic convex**

**When {% equation_inline K_G > 0 %}** (principal curvatures have the same sign):
- {% equation_inline H > 0 %}: **Convex** (elliptic)
- {% equation_inline H < 0 %}: **Concave** (elliptic)

**When {% equation_inline K_G < 0 %}** (principal curvatures have opposite signs):
- **Saddle** (hyperbolic)

### Recovering Principal Curvatures

Given {% equation_inline H %} and {% equation_inline K_G %}, we can recover the principal curvatures:

{% equation %}
K_{\min}, K_{\max} = H \pm \sqrt{H^2 - K_G}
{% endequation %} 

  
# Session 5: Mesh Smoothing and Denoising

## Introduction to Smoothing

Smoothing (or denoising) is a fundamental operation in geometry processing that reduces high-frequency noise while preserving the overall shape of a mesh.

## Laplacian Smoothing

The **Laplacian operator** is often called the "umbrella operator" because it averages a vertex's position with its neighbors in a way that resembles an umbrella.

We denote {% equation_inline L(v_i) %} as the discrete Laplacian of vertex {% equation_inline v_i %}.

### Mesh Representation

These methods work for any type of mesh. A mesh is represented as {% equation_inline M = (G, P) %} where:

- {% equation_inline G %} is the **mesh graph** describing how vertices are connected
- {% equation_inline P \in \mathbb{R}^{n \times 3} %} is the **geometry**, where {% equation_inline n %} is the number of vertices

The graph {% equation_inline G = (V, E) %} consists of:
- Vertex set: {% equation_inline V = \{i \mid 1 \leq i \leq n\} %}
- Edge set: {% equation_inline E \subset V \times V %}

### 1-Ring Neighborhood

The **1-ring neighborhood** of vertex {% equation_inline i %} is defined as:

{% equation %}
N(i) = \{j \mid (i, j) \in E\}
{% endequation %}

This is the set of all vertices directly connected to vertex {% equation_inline i %}.

### The Discrete Laplacian

The discrete Laplacian at vertex {% equation_inline v_i %} is:

{% equation %}
L(v_i) = \frac{1}{|N(i)|} \sum_{j \in N(i)} (v_j - v_i)
{% endequation %}

This computes the average displacement from {% equation_inline v_i %} to all its neighbors.

## Iterative Laplacian Smoothing

### Basic Update Rule

The basic iterative update rule is:

{% equation %}
v_i' \leftarrow v_i + \lambda L(v_i) \quad \text{where } \lambda \in (0, 1]
{% endequation %}

**Choosing {% equation_inline \lambda %}:** There is a trade-off to consider:
- **Large {% equation_inline \lambda %}:** Larger steps, faster smoothing, but risk of instability
- **Small {% equation_inline \lambda %}:** Smaller steps, avoids oscillations, but slower convergence

### Volume Loss Problem

The basic Laplacian smoothing has a significant drawback: **volume loss**.

- For **convex vertices**: The smoothing operation moves vertices inward, **losing area/volume**
- For **concave vertices**: The operation moves vertices outward, **gaining area/volume**

In closed shapes, there are typically more convex regions than concave ones, resulting in a **net volume loss**.

### Improved Update Methods

#### Bilaplacian Smoothing

The bilaplacian method takes one step forward and one step backward:

{% equation %}
\begin{aligned}
v_i' &\leftarrow v_i + \lambda L(v_i) \\
v_i'' &\leftarrow v_i' - \lambda L(v_i')
\end{aligned}
{% endequation %}

This method loses less volume than the basic Laplacian.

#### Taubin's Lambda-Mu Method

Taubin modified the bilaplacian approach by using different step sizes:

{% equation %}
\begin{aligned}
v_i' &\leftarrow v_i + \lambda L(v_i) \\
v_i'' &\leftarrow v_i' + \mu L(v_i')
\end{aligned}
{% endequation %}

The parameters are chosen such that {% equation_inline \frac{1}{\lambda} + \frac{1}{\mu} \approx 0.1 %}.

Solving for {% equation_inline \mu %}:

{% equation %}
\mu = \frac{-10\lambda}{10 - \lambda}
{% endequation %}

This formulation provides better volume preservation.

## Matrix Form of Laplacian Smoothing

We can express the Laplacian operator in matrix form, which opens the door to more sophisticated smoothing approaches.

### Matrix Definition

The Laplacian matrix is defined as:

{% equation %}
L = M^{-1} C
{% endequation %}

where:
- {% equation_inline C %} is the **cotangent Laplacian matrix** (also called the "weak Laplacian")
- {% equation_inline M %} is the **mass matrix**, which accounts for the local sampling density

### Cotangent Laplacian Matrix

The entries of {% equation_inline C %} are:

{% equation %}
(C)_{ij} = \begin{cases}
W_{ij} & \text{if } i \neq j \text{ and } (i,j) \in E \\
-\sum_{k \in N(i)} W_{ik} & \text{if } i = j \\
0 & \text{otherwise}
\end{cases}
{% endequation %}

**Weight choices:**

1. **Uniform Laplacian:** {% equation_inline W_{ij} = 1 %}

2. **Cotangent Laplacian:** {% equation_inline W_{ij} = \frac{1}{2}(\cot \alpha_{ij} + \cot \beta_{ij}) %}
   
   where {% equation_inline \alpha_{ij} %} and {% equation_inline \beta_{ij} %} are the angles opposite to edge {% equation_inline (i,j) %} in the two adjacent triangles.
   
   The cotangent weights try to preserve the distribution of the sampling and are more geometrically accurate.

### Mass Matrix

The mass matrix {% equation_inline M %} accounts for local vertex density:

**For uniform Laplacian:**

{% equation %}
(M)_{ij} = \begin{cases}
|N(i)| & \text{if } i = j \\
0 & \text{otherwise}
\end{cases}
{% endequation %}

**For cotangent Laplacian:**

{% equation %}
(M)_{ij} = \begin{cases}
A_i & \text{if } i = j \\
0 & \text{otherwise}
\end{cases}
{% endequation %}

where {% equation_inline A_i = \frac{1}{3} \sum \text{(areas of triangles adjacent to } v_i\text{)} %}

### Matrix Update Rule

With the matrix formulation, the update becomes:

{% equation %}
P' = P + \lambda L P
{% endequation %}

where:
- {% equation_inline P, P' \in \mathbb{R}^{n \times 3} %} (vertex positions before and after smoothing)
- {% equation_inline L \in \mathbb{R}^{n \times n} %} (Laplacian matrix)
- {% equation_inline \lambda \in (0, 1] %} (step size)

This can be written as:

{% equation %}
P' = (I + \lambda L) P
{% endequation %}

### Bilaplacian in Matrix Form

The bilaplacian smoothing becomes:

{% equation %}
\begin{aligned}
P' &= P + \lambda L P \\
P'' &= P' - \lambda L P' \\
&= (P + \lambda L P) - \lambda L(P + \lambda L P) \\
&= P + \lambda L P - \lambda L P - \lambda^2 L^2 P \\
&= (I - \lambda^2 L^2) P
\end{aligned}
{% endequation %}

## Global Smoothing Approach

### Motivation

The Laplacian {% equation_inline LP %} computes an update vector for each vertex. The magnitude of this vector is larger when vertices are farther from their neighbors.

Ideally, for a perfectly smooth surface, we would have {% equation_inline LP' \approx 0 %}.

However, {% equation_inline LP' = 0 %} has a trivial solution: {% equation_inline P' = 0 %} (all vertices collapse to the origin), which is not useful.

**Goal:** Keep the update vectors as close to zero as possible while maintaining the overall shape.

### Constrained Laplacian Smoothing

We divide vertices into two groups:

1. **Smooth vertices** {% equation_inline \{v_i \mid 1 \leq i \leq m\} %}: These can move freely
2. **Constraint vertices** {% equation_inline \{v_i \mid m < i \leq n\} %}: These are fixed

**Constraints:**

{% equation %}
\begin{aligned}
L(v_i') &= 0 \quad \text{for } 1 \leq i \leq m \\
v_i' &= v_i \quad \text{for } m < i \leq n
\end{aligned}
{% endequation %}

This is an interpolatory approach: smooth the interior while keeping boundary vertices fixed.

### Matrix Formulation

Let {% equation_inline L_1 %} be the first {% equation_inline m %} rows of {% equation_inline L %} (corresponding to smooth vertices).

We want to solve:

{% equation %}
\begin{pmatrix}
L_1 \\
I
\end{pmatrix} P' = \begin{pmatrix}
0 \\
P_{\text{constrained}}
\end{pmatrix}
{% endequation %}

This system has {% equation_inline n + m %} equations and {% equation_inline n %} unknowns.

We can solve this as a least squares problem:

{% equation %}
\begin{pmatrix}
L_1^T & I
\end{pmatrix} \begin{pmatrix}
L_1 \\
I
\end{pmatrix} P' = \begin{pmatrix}
L_1^T & I
\end{pmatrix} \begin{pmatrix}
0 \\
P_{\text{constrained}}
\end{pmatrix}
{% endequation %}

### Weighted Smoothing vs. Constraints

We can introduce a weight {% equation_inline \mu %} to balance smoothness and constraint satisfaction:

{% equation %}
\begin{pmatrix}
\lambda L \\
\mu I
\end{pmatrix} P' = \begin{pmatrix}
0 \\
\mu P
\end{pmatrix}
{% endequation %}

Larger {% equation_inline \mu %} emphasizes constraint satisfaction, while larger {% equation_inline \lambda %} emphasizes smoothness.

## Detail-Preserving Smoothing

We can decompose the mesh into low-frequency (smooth) and high-frequency (detail) components.

### Multi-scale Decomposition

Apply different numbers of smoothing iterations:

{% equation %}
\begin{aligned}
P_{\text{smooth}} &= (I + \lambda L)^{k_1} P \quad \text{(fewer iterations)} \\
P_{\text{low}} &= (I + \lambda L)^{k_2} P \quad \text{(more iterations, } k_2 > k_1\text{)}
\end{aligned}
{% endequation %}

### Detail Transfer

Reconstruct with controllable detail level:

{% equation %}
P' = P_{\text{low}} + \mu (P_{\text{smooth}} - P_{\text{low}})
{% endequation %}

**Effect of {% equation_inline \mu %}:**
- {% equation_inline \mu = 1 %}: {% equation_inline P' = P_{\text{smooth}} %} (preserve medium-frequency details)
- {% equation_inline \mu < 1 %}: Attenuate high-frequency details (more smoothing)
- {% equation_inline \mu > 1 %}: Exaggerate high-frequency details (enhance features)


# Session 6: Mesh Parametrization

## Introduction to Parametrization

**Mesh parametrization** is the process of creating a mapping between a 3D surface and a 2D parameter domain. This is essential for many geometry processing tasks, such as texture mapping, detail transfer, and surface analysis.

### Motivation: Attaching Properties to Surfaces

How do we attach properties (like colors, textures, or other attributes) to a surface? The typical approach is to **unfold the mesh** into a 2D domain, which allows us to work in a simpler parameter space.

### The Challenge: Distortion

When we unfold a 3D mesh, we inevitably introduce **distortion**, except in special cases involving developable surfaces (geometric objects that can be unfolded without stretching, such as cylinders and cones).

## Mathematical Framework

### Parametric Representation

A mesh is a discrete approximation of a surface embedded in {% equation_inline \mathbb{R}^3 %}. A parametric surface can be represented as:

{% equation %}
\mathbf{f}(u, v) = \begin{pmatrix} f_x(u, v) \\ f_y(u, v) \\ f_z(u, v) \end{pmatrix}
{% endequation %}

where {% equation_inline (u, v) %} are the 2D parameter coordinates, and {% equation_inline \mathbf{f}(u, v) %} gives the corresponding 3D position.

### Partial Derivatives

The partial derivatives {% equation_inline \frac{\partial \mathbf{f}}{\partial u} %} and {% equation_inline \frac{\partial \mathbf{f}}{\partial v} %} represent:

- **Tangent vectors** along the {% equation_inline u %} and {% equation_inline v %} parameter directions
- The local geometry and orientation of the surface
- How the surface "stretches" in different directions

**In CAD systems:** The function {% equation_inline \mathbf{f} %} is often composed of polynomials or rational functions (e.g., NURBS surfaces).

## Output of Parametrization Algorithms

A mesh parametrization algorithm produces:

- **UV coordinates** for each vertex (or corner) of the mesh
- A mapping from 3D mesh vertices to 2D parameter space

### Handling Discontinuities

**Challenge:** It is difficult to preserve continuity when flattening complex surfaces.

For example, when parametrizing a cube:
- Vertices must be "flattened out" onto the 2D domain
- Each triangle should map to a region in the UV space
- It's often impossible to maintain a single continuous parametrization

**Solution:** We introduce **discontinuities** (seams or cuts) and assign multiple UV coordinates to some vertices. This creates a **UV atlas** with potentially disconnected regions.

## Types of Bijectivity

A parametrization can be bijective at different levels:

### Global Bijectivity

The entire mapping {% equation_inline \mathbf{f}: D \to S %} is one-to-one and onto, where:
- {% equation_inline D \subset \mathbb{R}^2 %} is the parameter domain
- {% equation_inline S \subset \mathbb{R}^3 %} is the 3D surface

**Meaning:** Every point on the surface corresponds to exactly one point in the parameter domain, and vice versa.

### Local Bijectivity

The mapping is bijective in a small neighborhood around each point, but may not be globally bijective.

**Meaning:** Locally, the parametrization doesn't fold or overlap, but the global structure might have multiple charts or seams.

## Topological Equivalence

For a continuous function {% equation_inline \mathbf{f}: S_1 \to S_2 %}:

Two surfaces {% equation_inline S_1 %} and {% equation_inline S_2 %} are **topologically equivalent** (or homeomorphic) if there exists a mapping {% equation_inline \mathbf{f} %} such that:

1. {% equation_inline \mathbf{f} %} is **continuous**
2. {% equation_inline \mathbf{f}^{-1} %} (the inverse mapping) is **continuous**
3. {% equation_inline \mathbf{f} %} is a **bijection** (one-to-one and onto)

Such a mapping is called a **homeomorphism**.

**Key insight:** Topologically equivalent surfaces have the same fundamental structure (e.g., genus, number of holes), even if their geometric shapes differ.

## Applications of Mesh Parametrization

Parametrization is a fundamental technique used in many geometry processing applications:

### Mesh Morphing

Create smooth transitions between two 3D shapes by:
1. Parametrizing both meshes to a common 2D domain
2. Interpolating between corresponding UV coordinates
3. Reconstructing intermediate 3D shapes

### Detail Transfer

Transfer geometric or appearance details from one mesh to another:
1. Parametrize both source and target meshes
2. Use the UV mapping to establish correspondence
3. Transfer normal maps, displacement maps, or texture information

### Mesh Completion

Fill holes or missing regions in a mesh:
1. Parametrize the region around the hole
2. Use the parametrization to guide surface interpolation
3. Generate new geometry that smoothly fills the gap

# Session 7 - Geometry Processing: Distortion, Isometries & Parametrization

## 1. Introduction — What is This Lecture About?

This lecture focuses on **measuring and controlling distortion** when mapping between 3D surfaces and 2D domains.

In general, in **geometry processing**, we often have a 3D mesh (a surface embedded in {% equation_inline R^3 %}) that we want to **“unfold” or parametrize** onto a 2D plane {% equation_inline (R^2) %}, while preserving certain geometric properties — like angles, areas, or lengths.

So in this lecture:

* We start from a **2D parameter domain** {% equation_inline (u, v) \in \mathbb{R}^2 %},
* and a **mapping function** {% equation_inline f(u, v) \rightarrow \mathbb{R}^3 %} that defines a 3D surface.

The goal is to understand **how the local geometry changes** under this mapping — that is, how much stretching, shearing, or compression occurs when going from 2D to 3D (or vice versa, when flattening a 3D surface to 2D).

{% alert warning %}
This is meant o be applyed in objects that are topologically equivalent to a disc.

### The Goal of This Lecture (Session 7)

**how to analyze and classify mappings between surfaces**, so that you can later:

* **Decide what kind of parameterization** (flattening or texture mapping) is appropriate for your surface {% equation_inline S %},
* **Quantify distortion** (angle, area, or length distortion),
* **Choose the right mathematical tool** (isometric, conformal, or area-preserving mapping).

---

You’re taking a **mapping function**:
{% equation %}
f : (u,v) \in \mathbb{R}^2 ;\longrightarrow; \mathbf{x} \in \mathbb{R}^3
{% endequation %}
and, by examining its **Jacobian** {% equation_inline J %}, understand what kind of geometric distortion it introduces.

- Step 1: Compute the **Jacobian** {% equation_inline J %}
- Step 2: Compute the **First Fundamental Form**
- Step 3: Use **SVD decomposition** of {% equation_inline J %}
- Step 4: Interpret {% equation_inline \alpha_1, \alpha_2 %}

---


### What That Means Conceptually

You’re building the mathematical **intuition and tools** to look at a surface and say:

> “If I want to flatten this surface, what’s the best possible mapping I can use —
> one that keeps lengths, angles, or areas as close as possible to the original?”

So yes — the ultimate purpose is to **detect or choose** the correct type of transformation for your surface ( S ), based on:

* Its **geometry** (developable or not),
* Its **topology** (disk-like, sphere-like, with holes),
* And what property you care about preserving (angles, areas, or lengths).
{% endalert %}

---

## 2. Continuous Mapping and the Jacobian

We describe the local behavior of the mapping {% equation_inline f(u, v) %} using its **Jacobian matrix**, {% equation_inline J %}:

{% equation %}
J =
\begin{bmatrix}
\frac{\partial f_x}{\partial u} & \frac{\partial f_x}{\partial v} \\
\frac{\partial f_y}{\partial u} & \frac{\partial f_y}{\partial v} \\
\frac{\partial f_z}{\partial u} & \frac{\partial f_z}{\partial v}
\end{bmatrix}
{% endequation %}

This matrix describes how a small differential element in parameter space {% equation_inline (du, dv) %} transforms into 3D space. 
{% alert secondary %}
the matrix {% equation_inline J %} (the Jacobian of the mapping) is the gradient of the mapping function 
{% equation_inline f(u,v) %} — but expressed in matrix form.
It represents how the position in 3D changes with respect to small changes in 2D parameters.
{% endalert %}

The **metric tensor** {% equation_inline I %} (also called the **first fundamental form**) captures how lengths and angles are distorted:

{% equation %}
I = J^T J =
\begin{bmatrix}
E & F \\
F & G
\end{bmatrix}
{% endequation %}

where
- {% equation_inline E = f_u \cdot f_u %}
- {% equation_inline F = f_u \cdot f_v %}
- {% equation_inline G = f_v \cdot f_v %}

This tells us how the surface stretches or bends locally.

{% alert secondary %}
E, F, and G don’t each correspond directly to one specific type of distortion (angle or area) — instead, they jointly encode all distortions.

- E and G: describe stretching along coordinate directions
- F: describes angular (shear) distortion
- EG−F²: describes area distortion
{% endalert %}

---

## 3. Singular Value Decomposition (SVD) of the Jacobian

To analyze distortion, we apply **Singular Value Decomposition (SVD)** to {% equation_inline J %}:

{% equation %}
J = U \cdot \Sigma \cdot V^T
{% endequation %}

where:

*  {% equation_inline U %} and  {% equation_inline V %} are orthogonal matrices representing rotations,
*  {% equation_inline  \Sigma = \text{diag}(\sigma_1, \sigma_2) %} contains **singular values** (positive real numbers).

The singular values {% equation_inline \sigma_1, \sigma_2 %} describe how much the surface stretches along the principal directions.

We can define:
{% equation %}
\alpha_1 = \sigma_1, \quad \alpha_2 = \sigma_2
{% endequation %}
and they represent **principal stretches**.

### Special Cases

| Type of Transformation  | Conditions on α₁, α₂     | Description                                                       |
| ----------------------- | ------------------------ | ----------------------------------------------------------------- |
| **Isometry**            | α₁ = α₂ = 1              | Perfect preservation of lengths and angles (rigid motion)         |
| **Conformal map**       | α₁ = α₂                  | Angle-preserving, but may change area uniformly                   |
| **Area-preserving map** | α₁ * α₂ = 1              | Keeps areas constant, but angles may distort                      |
| **Developable surface** | Gaussian curvature K = 0 | Surface can be flattened without stretching (e.g. cylinder, cone) |


{% figure size="1" cols="1" caption="Type of transformations example" %}
/assets/Posts/GPR/deformation/deformation.gif
{% endfigure %}
---

## 4. Isometries and Developable Surfaces

### Isometry

If both stretches are 1, the mapping preserves both **lengths and angles**.
Isometries correspond to **rigid motions** — translations, rotations, and reflections.

### Developable Surfaces

Developable surfaces are 3D surfaces that can be unfolded to a plane **without distortion** (isometric flattening).
They satisfy **Gaussian curvature {% equation_inline K = 0 %}**.

Examples:

* Cylinder (without caps)
* Cone

Non-developable surfaces (like the saddle) have negative Gaussian curvature (K < 0).
Even though they contain straight lines — in fact, two families of them intersecting at each point — the surface cannot be flattened isometrically because it bends in opposite directions along those lines.

---

## 5. Discrete Case — Mesh Parametrization

When working with **triangle meshes**, we want to find a 2D parametrization of the vertices that minimizes distortion.

Given a triangle with vertices {% equation_inline P, Q, R %} in 3D and their corresponding 2D coordinates {% equation_inline P', Q', R' %}, the local transformation {% equation_inline M %} that maps 3D edges to 2D edges can be written as:

{% equation %}
M [R - P, Q - P] = [R' - P', Q' - P']
{% endequation %}

or equivalently:

{% equation %}
M = M_2 M_1^{-1}
{% endequation %}
where
{% equation %}
M_1 = [Q - P, R - P], \quad M_2 = [Q' - P', R' - P']
{% endequation %}

We can again use {% equation_inline M^T M %} to study local distortion — its eigenvalues correspond to the squares of the singular values {% equation_inline \sigma_1^2, \sigma_2^2 %}.

---

## 6. Conformal Maps

A **conformal map** preserves angles but not necessarily areas.

Mathematically, conformality requires that the **Jacobian satisfies**:
{% equation %}
f_v = f_u \times n
{% endequation %}
where {% equation_inline n %} is a rotation by 90° in the tangent plane.

This implies:
{% equation %}
\alpha_1 = \alpha_2
{% endequation %}

### Examples of Conformal Transformations:

* Translations
* Rotations
* Uniform (isotropic) scalings
* Reflections

---

## 7. Area-Preserving Transformations

For an **area-preserving** mapping:
{% equation %}
\det(J) = \alpha_1 \alpha_2 = 1
{% endequation %}

That means the **local area** of the parameter domain and the corresponding patch on the surface are equal.

---

## 8. Balancing Distortions — Energy Minimization

In practice, exact isometries or perfect conformal/area-preserving maps are rare.
Instead, we define **energy functions** to measure distortion and minimize them.

Example energy functions:

* **Conformal distortion energy:**

{% equation %}
E_c(\alpha_1, \alpha_2) = \mu_c (\alpha_1 - \alpha_2)^2
{% endequation %}

→ minimized when α₁ ≈ α₂.

* **Area distortion energy:**
{% equation %}
  E_a(\alpha_1, \alpha_2) = \mu_a \left( \frac{\alpha_1}{\alpha_2} + \frac{\alpha_2}{\alpha_1} \right)
{% endequation %}
→ minimized when α₁ * α₂ ≈ 1.

Different parametrization methods optimize different combinations of these.

---

## 9. Historical Approach — Tutte’s Embedding

**Tutte (1963)** proposed one of the first parametrization algorithms:

> “How to draw a graph”

* Works for **planar graphs**.
* Fixes boundary vertices to a convex polygon.
* Interior vertices are placed at the **barycenter** of their neighbors, minimizing a “spring energy”.
* Guarantees **no intersections**, but produces **poor parametrizations** (large distortions).

Later methods improved Tutte’s approach by adjusting edge weights or using curvature-aware Laplacians.

---

## 10. Mean Value Coordinates (Floater, 2003)

To improve upon simple Laplacian weights, **Mean Value Coordinates (MVC)** are used.

Given a vertex {% equation_inline v_0 %} connected to neighbors {% equation_inline v_i %}:
{% equation %}
w_i = \frac{\tan(\alpha_{i-1}/2) + \tan(\alpha_i/2)}{||v_i - v_0||}
{% endequation %}
and normalized as:
{% equation %}
\sigma_i = \frac{w_i}{\sum_j w_j}
{% endequation %}

This gives:
{% equation %}
v_0 = \sum_i \sigma_i v_i, \quad \sum_i \sigma_i = 1
{% endequation %}

MVC preserves **angle properties** better and provides **stable parametrizations** even for non-convex boundaries.

---

## 11. Summary Table

| Property                | Condition   | Preserves             | Examples                     |
| ----------------------- | ----------- | --------------------- | ---------------------------- |
| **Isometric**           | α₁ = α₂ = 1 | Lengths, Angles, Area | Rigid motion                 |
| **Conformal**           | α₁ = α₂     | Angles only           | Rotations, uniform scaling   |
| **Area-preserving**     | α₁ * α₂ = 1 | Area only             | Shear or anisotropic scaling |
| **Developable surface** | K = 0       | Locally flattenable   | Cylinder, Cone               |

---

## 12. Key Takeaways

* The **Jacobian** and its **SVD** describe how a 2D patch is distorted when mapped to 3D.
* **Isometries** are ideal but rarely achievable for curved surfaces.
* **Conformal** and **area-preserving** mappings are relaxations used in practice.
* Parametrization algorithms (like Tutte or Floater) try to **minimize distortion energies** for meshes.
* **Developable surfaces** {% equation_inline K = 0 %} can be unfolded without stretching — most real surfaces are not.

---


# Session 8
- Distorsion singular values of A.
- Isometries, angle preserving (conformal)
  - cueo preserving (authalic) ?
- which surfaces admit isometric R
  - K==0 <-> developable surfaces
- Mix and match prop. (energy opt)
- MVC

---

- In depth analysis of planar param.
  - optimization goal: 
  - by alpha cordin ?
  - quality guarantees
  - cost

Last day we mentioned that Tuttes result is connected to the quality guarantees because a convex convination always produces bijections... (i dont actually know what he is refering to)
{% equation %}
M = (V,E,F) -> f(p_i) = U_i belonging to D in R²
{% endequation %}

V are the {P_i}
E are the edges {(i,j)}
D can be any area like a circle or square or arbitrary surface.

We have a boundary {1,...,n}=interior and {n+1,...,N}=derivave M
N_i = {j|(i,j) in E}


U_i for i in Interior points = 1/(sum w_i*j where i belongs to N_i) * (sum w_i*j*U_j where j belongs to N_i)

<!-- Here he lost me a bit: -->
U is the surface composed of (M_i, N_i). we can create matrix A * [M_1, M_2...].T that is the same as B [U_{n},...,U_N]

a_{ij} is:
- sum w_{ik} if n=i
- -w_{ij} if i != j and (i,j) belongs to E
- 0 otherwise

A will only depend on W_{ij}. B will be W_{ij} and is multiplying fixed delta schemes.

The selection of points in boundary and what can be expected. We have the points in R³, i draw a curve that is a polygon and i can compute the lenght L of the hole poligon as the sum || p_i-p_{something} ||^{alpha} where i is {N_1..., N}

Now for the other points i choose an image for the next point and i compute the distance to the boundary
d_j = sum ||p_i-p_{something}||^{alpha}

d_j/L belongs to [0,1]

d_i/L * P; (P is perimeter)

--- 
<!-- Second part of session 8 -->
Lets draw points  and we add a spring between the points. We can calculate how much they are being pressed or separated with E=1/l * k * s². We name this srings k

With this information we get that 
E = 1/2 (sum 1/2 * k_{ij} ||p_i - p_j||²) = 1/4 (sum k_{ij*(pi_pj).T*(pi-pj)})

delta E / delta M_i = 1/4 (sum k_{ij} 2*(p_i-p_j)) ? not finished, i assume it is summing all the springs or something.


Linear reoriducibility: Tutte only cares about geometry (the connection of points) moving the non boundary points. This does not have reproducibility. 

This means that A scheme will have reproducibility only if it represents the baricenter coordinates.

Baricenter coordinated for poligons
- cotangent weights (harmonic params)
- w_ij = (ctan(alpha) + cotan(beta)) / 2
- cotangent weights can be negative, diferently to tutte, we can not assure to have a 1 to 1 parametrization.

Waclispaces coordinates
- w_ij = (cot(alpha) + cot(beta)) / || pi-pj ||

eigendecomposition, less square, laplacian.

<!-- For some reasom, this is meant to be strongly relationated with laplacian  -->


# Session 9
<!-- Last day we saw Analysis of scheme and based affine combinations. We also explored the NN spring model.

- Linear reproduction
<-> scheme == baryanlic coro?
p_i = sum(j in d(i), w_ij * p_i) for triangle uniq...

Now we explore:
* Cotangent weights:
  * harmonic weights
  * + Matrix is non uniform
  * + small angle dist
  * - negative weights
* wachpress weights
  * in two connected triangles on an edge:
    w_j = (cot \alpha + cot \beta / ||p_i-p_j||) 
    where:
      - p, p_j, d, p_{j+1} are angles
    
    w_j = B_j / (A_{i-1} * A_j)
We caclulate the varicentre to recalculate the point b?

B_j*P = A_j * p_{i-1} = (A_{i-1}+A_j-B_i)*P_j + A_{i-1}
= A_j (P_{j-1} - P_j) - A_{j-1}(P_j-P_{j+1}) + B_j *\phi_i


sum(j, B_i / A_j*A_{j-1})(p-p_j)=0

We still need to discuss somethings like three boundary points to make use of this section.

- MVC: floater
Do we have some free boundary?


- **Most isometric Paom**
  - start with a guess (with good properties) usually fixed derivative.
  - Iterativelly improve past of "worst" point  (int ac derivative) 



NEXT TOPIC: REMESHING
WE need a metric that tells us what is better "mesh"
- types of primitives
  - triangle, square, hexagonal...
-->

“Last day we looked at linear schemes for parameterization, affine combinations, and the ‘spring model’ (Tutte).

The important condition from last time is:”

### **Linear reproduction**

We want a coordinate scheme that satisfies:

{% equation id="linear-reproduction" %}
p_i = \sum_{j \in N(i)} w_{ij} \, p_j
{% endequation %}

for **any** *affine* function {% equation_inline p %}.
This means the weights reproduce linear functions exactly.
This is the definition of **barycentric coordinates** on a mesh.

> “If your weights reproduce linear functions, the scheme is affine, and therefore suitable for mesh parameterization.”

{% alert secondary %}
**Reminder**: What are weights?
Weights {% equation_inline w_{ij} %} are scalars assigned to each edge {% equation_inline (i,j) %} used to express an interior vertex (or point) as an affine/barycentric combination of its neighbors:
{% equation_inline p_i = \sum_{j \in N(i)} \sigma_{ij} \, p_j %}, with {% equation_inline \sigma_{ij} = w_{ij} / \sum_k w_{ik} %} (normalization)



{% endalert %}

---

## **2. Today: Three families of barycentric weights**

Today’s lecture surveys **three classic weight constructions** used in mesh parametrization:

1. **Cotangent weights** (a.k.a. harmonic weights)
2. **Wachspress coordinates**
3. **Mean Value Coordinates (MVC)** by Floater

For each:

* why it's good
* why it's bad
* where it comes from
* how it affects parameterization quality

---

## **3. Cotangent weights**

These arise from discrete differential geometry.
They are connected to the Laplace–Beltrami operator.

### **Definition (for a triangulated mesh)**

For an interior edge {% equation_inline i \leftrightarrow j %}:

{% equation id="cotangent-weights" %}
w_{ij} = \frac{1}{2} \left( \cot \alpha_{ij} + \cot \beta_{ij} \right)
{% endequation %}

where {% equation_inline \alpha_{ij} %} and {% equation_inline \beta_{ij} %} are the angles opposite the edge {% equation_inline (i,j) %} in the two adjacent triangles.

These are the weights used in **harmonic maps** and **discrete Laplacian**.

### **Properties**

**+ Very good angle behavior**
They minimize Dirichlet energy and give low angle distortion.

**+ Produce very smooth parameterizations**

**– Can produce negative weights**
If a triangle has an obtuse angle, the cotangent of an obtuse angle is negative.
Negative weights break injectivity → possible fold-overs in parameterization.

**– Matrix is non-uniform**
The Laplacian matrix with cot weights is symmetric but not uniform, and not always diagonally dominant.

> “Cotangent weights are great, but dangerous. They are the gold standard for geometry processing, but you have to watch the signs.”

---

## **4. Wachspress coordinates**

These originate from projective geometry.
They are generalized barycentric coordinates on convex polygons.

Consider a convex polygon with vertices {% equation_inline p_1, p_2, \ldots, p_n %}.
For each vertex {% equation_inline p_j %}, the **Wachspress weight** at interior point {% equation_inline p %} is:

{% equation id="wachspress-weights" %}
w_j(p) = \frac{B_j(p)}{A_{j-1}(p) \, A_j(p)}
{% endequation %}

Where:

* {% equation_inline A_j %} = signed area of triangle {% equation_inline (p, p_j, p_{j+1}) %}
* {% equation_inline B_j %} = area of the quadrilateral formed by edges meeting at vertex {% equation_inline j %}

These areas appear in formula:

{% equation id="wachspress-simplified" %}
w_j = \frac{B_j}{A_{j-1}A_j}
{% endequation %}

This matches the Wachspress construction.

### **Properties**

**+ Always positive (for convex polygons)**
No negative weights → preserves injectivity.

**+ Coordinate functions are rational**
Smooth inside, but more complex.

**– Only guaranteed to work on convex polygons**
Not useful on arbitrary boundary shapes.

---

## **5. Where the “baricenter equation”  comes from**

This is the professor deriving the **linear precision condition**:

{% equation id="linear-precision" %}
\sum_j w_j(p) \, p_j = p
{% endequation %}

To prove Wachspress coordinates reproduce affine functions, he expresses the vector {% equation_inline p %} as a combination of neighbors using areas.
This long expression is the expanded form of that algebraic proof.

He likely said:

> “Don’t memorize this. Just understand: Wachspress coordinates satisfy linear precision because of the area-ratio construction.”

---

## **6. The “sum over j” formula**

I wrote:

```
sum(j, B_i / A_j*A_{j-1})(p-p_j)=0
```

This is shorthand for:

{% equation id="equilibrium-equation" %}
\sum_j w_j(p) \, (p - p_j) = 0
{% endequation %}

Which is the **equilibrium equation** for barycentric coordinates.
This is the key property: the weighted neighbors “pull” the point into equilibrium at (p).

---

## **7. MVC — Mean Value Coordinates (Floater)**

This was introduced because:

* cotangent weights have negative weights
* Wachspress works only for convex boundaries

MVC works on **any simple polygon**, convex or not.

### **Definition**

For each neighbor {% equation_inline j %}:

{% equation id="mvc-weights" %}
w_j(p) = \frac{\tan(\theta_{j-1}/2) + \tan(\theta_j/2)}{|p - p_j|}
{% endequation %}

Where {% equation_inline \theta_j %} is the angle between edges to {% equation_inline p_j %} and {% equation_inline p_{j+1} %}.

### **Properties**

**+ Always positive weights**
Works on non-convex polygons.

**+ Conformal-like behavior**
Angle-preserving tendencies (not exactly conformal, but close).

**– Not optimal for area distortion**
But usually much better than Tutte / springs.

Your professor might have said:

> “MVC behaves like a discrete harmonic map but without negative weights.”

---

## **8. Boundary conditions**

Barycentric coordinate systems typically require:

* at least 3 non-collinear boundary points to anchor the mapping (triangle boundary), or
* a fixed boundary polygon (arbitrary shape), or
* circular boundary (for Floater’s MVC disk embedding)

This is necessary to avoid the trivial solution {% equation_inline p_i = \text{constant} %}.

---

## **9. Most isometric parameterization (Lévy / Desbrun style)**


```
Most isometric Param
- start with a guess (fixed boundary)
- iteratively improve the "worst" point (intrinsic derivative)
```

This refers to **Most Isometric Parameterization (MIPS)** by Hormann & Greiner.

Key idea:

* Start with an initial mapping (e.g., MVC).
* Measure distortion at every triangle using singular values {% equation_inline \alpha_1, \alpha_2 %}.
* Improve the map by minimizing:

{% equation %}
E = \frac{\alpha_1}{\alpha_2} + \frac{\alpha_2}{\alpha_1}
{% endequation %}


That is the “worst distortion” the professor refers to.

The process:

1. Fix the boundary (common choice).
2. Compute Jacobian on each triangle.
3. Compute singular values.
4. Move vertices to reduce the energy.
5. Iterate until distortion minimized.

---

## **10. Summary — what you should take away**

Your professor wants you to understand:

### **Cotangent weights**

* Best differential properties
* May become negative → dangerous in parametrization

### **Wachspress coordinates**

* Exact barycentric coordinates on convex polygons
* No negative weights
* Not general enough

### **Mean Value Coordinates (MVC)**

* General-purpose, always positive
* Great for disk parameterization
* Foundation for many modern methods (LSCM, MIPS)

### **General principle**

All these schemes boil down to:

{% equation id="general-barycentric" %}
p_i = \sum_j w_{ij} p_j, \qquad \sum_j w_{ij} = 1, \qquad w_{ij} \ge 0
{% endequation %}

The goal is to find weights that produce:

* good angle behavior
* no fold-overs
* smooth interior
* no negative weights
* linear precision

---

# Session 10
We spoke about Remeshing. Specifically Delauray Delta for 2 and 3D.

There is a duality with Voronoi Diagram and Remeshing. 
- Delaray Delta:
  - 0 Dimensions: vertices
  - 1 dimensions: edges
  - 2 dimensions: triangles

- Voronoi Diagram:
  - 0 Dimensions: cells/regions 2D
  - 1 dimensions: edges (derivative of regions )
  - 2 dimensions: vertices of derivative of regions

Example of tetrahedro with
0 verts <-> cells 3 dim
1 edges <-> facets 2 dim
2 faces <-> edges of facets 1
3 tetra <-> cones of facets


## Delaunay and Voronoi (brief notes)

This section summarizes the core definitions and properties you will see in
remeshing and computational geometry. The presentation below uses classical
terminology and short formal definitions — keep this as a quick reference.

### Delaunay triangulation (2D)

Definition. Let {% equation_inline V \subset \mathbb{R}^2 %} be a finite point set. A triangle
{% equation_inline T %} with vertices in {% equation_inline V %} is a Delaunay triangle if the circumcircle of
{% equation_inline T %} contains no point of {% equation_inline V %} in its interior. A Delaunay triangulation
of {% equation_inline V %} is a triangulation of the convex hull of {% equation_inline V %} whose every triangle
is Delaunay.

Properties:

1. The Delaunay triangulation maximizes the minimum angle among all
   triangulations (it avoids skinny triangles whenever possible).
2. It is unique when no four points of {% equation_inline V %} are cocircular (no four points
   lie on the same circle).

Duality. The dual graph of the Delaunay triangulation is the Voronoi diagram
of {% equation_inline V %}. For a site {% equation_inline p \in V %} the Voronoi cell is defined as
{% equation id="voronoi2d" %}
\operatorname{cell}(p) = \{x\in\mathbb{R}^2:\; \|x-p\| \le \|x-q\|,\; \forall q\in V\}
{% endequation %}

The vertices of the Voronoi diagram are precisely the circumcenters of the
triangles in the Delaunay triangulation.

### Delaunay triangulation (3D)

Definition. For a finite point set {% equation_inline V \subset \mathbb{R}^3 %} a tetrahedron
{% equation_inline T %} with vertices in {% equation_inline V %} is Delaunay if its circumsphere has no points
of {% equation_inline V %} in its interior. A Delaunay tetrahedralization is a tetrahedral mesh
filling the convex hull of {% equation_inline V %} consisting only of Delaunay tetrahedra.

Properties:

1. Uniqueness holds when no five points of {% equation_inline V %} are cospherical.
2. The dual structure is the 3D Voronoi diagram; Voronoi vertices are the
   circumcenters of Delaunay tetrahedra.

### Constrained and conforming Delaunay (3D)

- Constrained Delaunay Triangulation (CDT): a CDT is defined with respect to a
  geometric domain (polyhedron) {% equation_inline X %}. A tetrahedron is *constrained Delaunay* if
  its circumsphere does not contain vertices of {% equation_inline X %} that are visible from the
  interior of the tetrahedron. CDTs respect input geometry (edges/faces) but
  may require additional Steiner points to exist.

- Conforming Delaunay tetrahedralization: a tetrahedralization where every
  tetrahedron is Delaunay; in practice this often requires adding Steiner
  points so that the triangulation both respects the domain and remains Delaunay.

### Delaunay restricted to a surface

Given points {% equation_inline V \subset \mathbb{R}^3 %} and a surface {% equation_inline S %}, the Delaunay
triangulation restricted to {% equation_inline S %} is the subset of Delaunay
faces whose dual Voronoi edges intersect the surface. This is the foundation of
restricted-Voronoi / restricted-Delaunay algorithms used for surface
remeshing.

### Performance notes

- Many practical Delaunay algorithms are incremental: points are inserted one
  at a time and local updates maintain the Delaunay property.
- Worst-case running time can be quadratic, but average expected complexity is
  typically {% equation_inline O(n\log n) %} in practice for well-behaved point distributions.

---
## Greedy Remeshing

## Incremental remeshing

# Session 11
## Model repair
Models that we handle in real life they will have defects such as holes. Depending on how was it geenrated it will have different errors and deformations.

If we have a  non manifold meshes, we must turn them into manifold. Remember that manifold surfaces have:
- does not have singular vertex
- does not have complex edges
- does not have intersections

Other problems we may face are:
- holes 
- gaps
- overlaps

Those mentioned are geometric problems. Now we go to topological problems
- Spurious handles: topological noise. Genus is not zero. This are called through holes
- Inconsistent face orientation.

Algorithms can be classified as:
<!-- TODO: check power point and add missing info -->
- Surface based
- Volume based

Volume Based algorithms can help with topological problems such as through-holes while surface based algorithms help more in broken meshes.

This algorithms are meant to work on localized problems and try not to touch the rest of the model.

Range scans: This method obtains surfaces from a scanned object. The problem is that this strategy produces lots of individual surfaces overlapped. We require either surface-based algorithms or volume based algoirthms capable of detecting overlaps and generate a single mesh.

Once we fuse those range scans using a surface-based method, we will generate other problems such as holes and isles.

### Triangle Soup
<!-- TODO: what type of algorithms is this? what problem does it solve? what limitations does it have. -->
Subdivision step into cells.

### Hole filling problem

# Laboratory Sessions
## Lab 1: Normal approximation
We are given a point cloud we will call P. We assume it represents a surface (I dont know what will happen if it was a volume). Our purpose is to calculate the normal direction of the surface in that point. 

We will interpret a collection of points as a set of connected vertices of a surface. We will use Principal Component Analysis to obtain the 3 orthonormal axis that best represent the subset of point clouds.

  {% figure id="multiple-figures" size="1.0" 
  caption="Select a point, get its kNN points, calculate center of mass, move it to the origin"
  col="1" %}
  /images/gpr/ezgif-239f0d3b2d317a3f.gif
  {% endfigure %}

We can now create a matrix C that will contain how much each point differs from the origin. 

### Covariance matrix C

Formula (centered points):

{% equation id="energy" %}
C=\sum_{i=1}^{n}\tilde p_i\,\tilde p_i^{T}\quad\text{with}\quad \tilde p_i=p_i-\bar p,\;\bar p=\frac{1}{n}\sum_{i=1}^n p_i
{% endequation %}

Eigendecomposition:
{% equation id="energy" %}
C = V \,\Lambda\, V^{T}
{% endequation %}

where columns of {% equation_inline V %} are orthonormal eigenvectors and {% equation_inline \Lambda %} is the diagonal matrix of eigenvalues.

Short explanation:
- \(C\) is the (unnormalized) covariance / scatter matrix of the centered points.  
- It encodes how the points are distributed around their mean; principal directions (PCA) are given by the eigenvectors, and eigenvalues measure variance along those directions.

Numeric example (3 points in R^3):
- Points:
{% equation id="energy" %}
p_1=(1,0,0),\; p_2=(0,1,0),\; p_3=(0,0,1)
{% endequation %}
- Mean:
{% equation id="energy" %}
  \bar p=(\tfrac{1}{3},\tfrac{1}{3},\tfrac{1}{3})
{% endequation %}
- Centered vectors:
{% equation id="energy" %}
  \tilde p_1=(\tfrac{2}{3},-\tfrac{1}{3},-\tfrac{1}{3}),
  \tilde p_2=(-\tfrac{1}{3},\tfrac{2}{3},-\tfrac{1}{3}),
  \tilde p_3=(-\tfrac{1}{3},-\tfrac{1}{3},\tfrac{2}{3}).
{% endequation %}
- Compute {% equation_inline C=\sum \tilde p_i\tilde p_i^T %} :
{% equation id="energy" %}
  C=\begin{pmatrix}
  \tfrac{2}{3} & -\tfrac{1}{3} & -\tfrac{1}{3}\\[4pt]
  -\tfrac{1}{3} & \tfrac{2}{3} & -\tfrac{1}{3}\\[4pt]
  -\tfrac{1}{3} & -\tfrac{1}{3} & \tfrac{2}{3}
  \end{pmatrix}
{% endequation %}


 - Eigen-decomposition (PCA):
   - Eigenvalues: {% equation_inline \{1,\,1,\,0\} %}.
   - Orthonormal eigenvectors (columns of {% equation_inline V %}), for example:
     v1 = (1,-1,0)/√2, v2 = (1,1,-2)/√6, v3 = (1,1,1)/√3.
   - So {% equation_inline C = V\,\mathrm{diag}(1,1,0)\,V^{T} %}.

This shows \(C\) captures principal directions (two directions with variance 1, one null direction along the mean vector).

To calculate the normal direction,  we will need a special library capable of solving for {% equation_inline C=VΛV^T %}. We will use 


{% highlight cpp linenos %}
// We use Eigen's SelfAdjointEigenSolver to compute the eigenvalues and eigenvectors
Eigen::SelfAdjointEigenSolver<Eigen::Matrix3f> eigensolver(cov_eigen);
if (eigensolver.info() == Eigen::Success) {
  Eigen::Vector3f eigenvalues = eigensolver.eigenvalues();
  Eigen::Matrix3f eigenvectors = eigensolver.eigenvectors();
  
  // Smallest eigenvalue corresponds to normal direction
  // we know eigenvalues are sorted in increasing 
  Eigen::Vector3f normal = eigenvectors.col(0); // First column
  
  // Convert back to GLM
  normals[i] = glm::normalize(glm::vec3(normal.x(), normal.y(), normal.z()));
}
{% endhighlight %}

## Lab 2: Iterative Closest Point
## Lab 3: Reconstruction

We will generate a mesh from a point cloud. We are instructed two methods:
- Simple reconstruction
- Radial Basis FUnctions (RBF)

### Simple reconstruction
```python
i ← Index of pi, closest point to p
{ Compute z as the projection of p onto the tangent plane at pi }
z ← pi − ((p − pi) · ni) · ni
if distance(z, pi) ≤ ρ + δ then
    f (p) ← (p − pi) · ni
else
    f (p) ← undef ined
end if
```




### Hoppe Abstract
We describe and demonstrate an algorithm that takes as input an unorganized set of points {x1,...,xn} belonging to R³ on or near an unknown manifold M, and produces as output a simplicial surface that approximates M. Neither the topology, the presence of boundaries, nor the geometry of M are assumed to be known in advance — all are inferred automatically from the data. This problem naturally arises in a variety of practical situations such as range scanning an object from multiple view points, recovery of biological shapes from two-dimensional slices, and interactive surface sketching.

We get input: Set of pairs {% equation_inline (p_i, v_i), p_i ∈ R^3, v_i ∈ R %}.

{% equation_inline p_i %}  is just the sampled points in the surface (point cloud). {% equation_inline v_i %}  is a scalar obtained with {% equation_inline f(p) ← (p − p_i) · n_i %} 


Using the input (p_i, v_i) we compute

{% equation %}
f(p) = \sum_{i=1}^{m} f_i(p), \quad f_i(p) = \phi(\|p - p_i\|)\,c_i
{% endequation %}

Definitions:

- {% equation_inline \phi %} — radial basis kernel: a scalar function of the radial distance {% equation_inline r %} that controls smoothness and support. It depends only on {% equation_inline r = norm(p - p_i) %}. Common choices:
  - Gaussian: {% equation_inline \phi(r) = exp(-r^2 / (2 sigma^2)) %}
  - Multiquadric: {% equation_inline \phi(r) = sqrt(r^2 + eps^2) %}
  - Inverse multiquadric: {% equation_inline \phi(r) = 1 / sqrt(r^2 + eps^2) %}
  - Thin-plate spline: {% equation_inline \phi(r) = r^2 * log(r) %}
  - Compactly supported (Wendland): {% equation_inline \phi(r) = (1 - r / R)^k_+ * polynomial %}
  The choice determines smoothness, support radius and numerical conditioning.

- {% equation_inline norm(p - p_i) %} — Euclidean ({% equation_inline l2 %}) distance between query point {% equation_inline p \in R^3 %} and center {% equation_inline p_i \in R^3 %}:
  {% equation_inline r = norm(p - p_i)_2 %}.

- {% equation_inline c_i %} — scalar coefficient (weight) associated to center {% equation_inline p_i %}. Coefficients are solved so {% equation_inline f %} interpolates or approximates the data:
  - Assemble {% equation_inline A \in R^{m x m} %} with {% equation_inline A_{ij} = \phi(norm(p_i - p_j)) %}.
  - Solve {% equation_inline A c = v %} for {% equation_inline c %} (or regularized: {% equation_inline (A + \lambda I) c = v %}) where {% equation_inline c = [c_1 ... c_m]^T %} and {% equation_inline v = [v_1 ... v_m]^T %}.

- {% equation_inline p_i %} — centers (sample positions) in {% equation_inline R^3 %}. Typically {% equation_inline m = n %} and centers coincide with the input samples, but you may choose a subset of centers.

- {% equation_inline v_i %} — target scalar values at centers (observations). In surface reconstruction from point+normal data these are often:
  - {% equation_inline v_i = 0 %} for on-surface samples,
  - additional synthetic off-surface samples {% equation_inline v_i = +- d %} (offset by normal) to impose normal constraints.
  More generally {% equation_inline v_i %} is the desired value {% equation_inline f(p_i) %}.

Notes:
- After solving for {% equation_inline c %}, evaluate {% equation_inline f(p) %} cheaply by summing {% equation_inline \phi(norm(p - p_i)) * c_i %}.
- Regularization ({% equation_inline lambda > 0 %}) improves conditioning for large {% equation_inline m %} or noisy data.
- Choosing compactly supported {% equation_inline \phi %} makes {% equation_inline A %} sparse and evaluation faster.
As m increases, the matrix A tends to become ill-conditioned. "Ill-conditioned" means the mathematical problem (typically a linear system or a matrix) is highly sensitive to small changes in the input data or to rounding errors. In practice:

- The determinant of A is close to zero and the condition number is very large.
- Solving A x = b becomes unstable: tiny changes in A or b can produce huge changes in x.
- This commonly occurs when the sample points (or basis functions) are nearly linearly dependent, are very close together, or when the system is near singular.

(Regularization such as solving (A + λI) c = v is a common mitigation.)

<!-- You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-title.MARKUP`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `MARKUP` is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:

{% highlight ruby %}
def print_hi(name)
  puts "Hi, #{name}"
end
print_hi('Tom')
#=> prints 'Hi, Tom' to STDOUT.
{% endhighlight %}

Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/ -->
