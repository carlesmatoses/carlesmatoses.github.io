---
layout: post
title:  "GPR Notes UPC"
date:   2025-09-15 12:00:00 +0200
preview: "/images/gpr/gpr.png"
categories: post
permalink: post/GPR
---

This are some of my notes for Geometry Processing. 
<!-- end-abstract -->

<!-- index -->
* Do not remove this line (it will not be displayed)
{:toc}

{% bibliography_loader _bibliography/ao_references.bib %}


# Context
-  Final Grade = 0.1 Class + 0.35 Lab + 0.4 Exam + 0.15 LabExam.

# Previous capacities
## vector spaces
## transformations
Linear transformations always preserve the origin {% equation_inline A0=0 %}

## change of bases
## eigenvalues and eigenvectors
`Eigenvectors` are special, non-zero vectors that do not change their direction when a linear transformation is applied to them, instead being scaled by a constant factor called an `eigenvalue`.

For {% equation_inline v %} to be an eigenvector of {% equation_inline A %}

{% equation id="for-v-to-be-eigenvector" %}
A v = \lambda v
{% endequation %}

with {% equation_inline  λ \in R %} and {% equation_inline v \ne 0 %} 

But the **why do we care?** is still fuzzy. Let’s make it concrete with examples from geometry processing and beyond.
1. Eigenvectors give special directions of a transformation

    Imagine you have a matrix {% equation_inline  A %} that transforms vectors. 
  
    In most directions, {% equation_inline  Av %} will change direction and length. But along eigenvectors, {% equation_inline  Av %} only changes length (scaling by {% equation_inline  \lambda %}).

    So:
      * Eigenvectors = “axes” of the transformation.
      * Eigenvalues = how much the transformation stretches/squashes along those axes.
    
    That’s the raw math meaning. Now — what do we do with that?

## SVD
Singular Value Decomposition (SVD) is a powerful matrix factorization technique that expresses any $m \times n$ matrix $A$ as the product $A = U \Sigma V^T$, where $U$ and $V$ are orthogonal matrices and $\Sigma$ is a diagonal matrix containing the singular values of $A$. SVD reveals the intrinsic geometric structure of the matrix, making it invaluable for applications such as dimensionality reduction, noise filtering, and solving linear systems, especially when $A$ is not square or is ill-conditioned. 

To find the singular values of a matrix {% equation_inline A %}, you must compute the square roots of the positive eigenvalues of the matrix {% equation_inline A^{T}A %} (or {% equation_inline AA^{T} %}). The general process involves calculating {% equation_inline A^{T}A %}, determining its characteristic polynomial by setting the determinant of ({% equation_inline A^{T}A-\lambda I %}) to zero, solving for the eigenvalues ({% equation_inline \lambda %}), and then taking the square root of each positive eigenvalue to find the singular values of  A.

## differential geometry of curves and surfaces:
## C++

# Geometry processing pipeline, (reconstruction)

- point clouds -> alignment -> point cloud -> normal estimation -> point cloud +normal -> Reconstruction -> surface
- Loss in each step


# Surfaces 
## Linear Algebra
Linear algebra is a branch of mathematics that studies linear equations, linear functions, and their representations through vectors and matrices. It provides the foundational language and tools for analyzing systems where relationships are linear, meaning they can be described by equations of the form 
{% equation_inline a_1x_1 + a_2x_2 + \cdots + a_nx_n = b %}. 
Linear algebra is essential in many fields, including science, engineering, computer graphics, and machine learning, as it allows for the efficient manipulation and understanding of multidimensional data and transformations.


**Eigendecomposition** is a method of expressing a square matrix in terms of its eigenvalues and eigenvectors. For a matrix $A$, if it can be written as $A = V D V^{-1}$, where $V$ contains the eigenvectors and $D$ is a diagonal matrix of eigenvalues, then $A$ is said to be diagonalizable. This decomposition reveals the fundamental directions (eigenvectors) and scaling factors (eigenvalues) of the transformation represented by $A$. Eigendecomposition is useful in simplifying matrix operations and understanding the behavior of linear transformations in geometry processing, physics, and data analysis.

{% equation id="energy" %}
A \in R^{n \times n}
{% endequation %}

For future examples, let’s use the following $3 \times 3$ matrix as $A$:
{% equation %}
A = \begin{pmatrix}
2 & 1 & 0 \\
1 & 2 & 1 \\
0 & 1 & 2
\end{pmatrix}
{% endequation %}

The formula $Av = \lambda v$ means that when you apply the matrix $A$ (a linear transformation) to the vector $v$ (an eigenvector), the result is just the same vector $v$ scaled by the number $\lambda$ (the eigenvalue). 

In other words, $v$ is a special direction for $A$: instead of changing direction, $A$ only stretches or shrinks $v$ by the factor $\lambda$. The matrix $A$ does not "store" the eigenvalues; rather, the eigenvalues and eigenvectors are special solutions to this equation for a given $A$.

<!-- eigenvector: 
    
    Av=λv

    A is a linear transformation matrix
    v is a nonzero vector
    λ is a scalar.

    This equation says: when you apply   -->
<!-- 
v eigenvector of A <=> Av=lambda*v where v not equal zero and v belongs to R^{n} and lambda belongs to R and ||v||=1
Lambda eigenvector of A corresponding to v -->

<!-- Av = lambda v -> Av-lambda v = 0 -> Av -lambda Iv = 0 -> (A-lambdaI)v=0
A-lambdaI = 0 -->

We can rewrite the eigenvector equation step by step:
{% equation %}
Av = \lambda v \\
Av - \lambda v = 0 \\
Av - \lambda I v = 0 \\
(A - \lambda I)v = 0
{% endequation %}

To have a nontrivial solution ($v \neq 0$), the matrix $(A - \lambda I)$ must be singular (not invertible). This happens only when its determinant is zero:
{% equation %}
\det(A - \lambda I) = 0
{% endequation %}
This is called the characteristic equation. Solving it gives the eigenvalues $\lambda$ of $A$. For each eigenvalue, you can then find the corresponding eigenvectors $v$ by solving $(A - \lambda I)v = 0$.

<!-- 
A symetric -> lambda' s belongs to R
              v' s belongs to R^{n}
              v' s linearly independent
              v orthogonal (i can get an orthonormal system?)

These are:
Characteristic equation of A
characteristic polynomial of A 
-->

The **characteristic polynomial** of a matrix $A$ is the polynomial you get when you compute $\det(A - \lambda I)$, where $I$ is the identity matrix and $\lambda$ is a variable. It is called a polynomial because, for an $n \times n$ matrix, expanding the determinant gives a degree $n$ polynomial in $\lambda$. The roots of this polynomial are the eigenvalues of $A$. In summary:

{% equation %}
	ext{Characteristic polynomial:}\quad p(\lambda) = \det(A - \lambda I)
{% endequation %}
Solving $p(\lambda) = 0$ gives all the eigenvalues of $A$.


#### 1. Subtract $\lambda$ from each diagonal entry to get $A - \lambda I$: 
  {% equation %}
  A - \lambda I = \begin{pmatrix}
  2 - \lambda & 1 & 0 \\
  1 & 2 - \lambda & 1 \\
  0 & 1 & 2 - \lambda
  \end{pmatrix}
  {% endequation %}

#### 2. Compute the determinant and set it to zero: 
  {% equation %}
  \det(A - \lambda I) = 
  \begin{vmatrix}
  2 - \lambda & 1 & 0 \\
  1 & 2 - \lambda & 1 \\
  0 & 1 & 2 - \lambda
  \end{vmatrix}
  = 0
  {% endequation %}

#### 3. Expand the determinant: 
   {% equation %}
  (2-\lambda)\left[(2-\lambda)(2-\lambda) - 1\cdot1\right] - 1\left[1(2-\lambda) - 1\cdot0\right]
  {% endequation %}
  which simplifies to:
  {% equation %}
  (2-\lambda)\left[(2-\lambda)^2 - 1\right] - (2-\lambda)
  {% endequation %}
  {% equation %}
  = (2-\lambda)\left[(2-\lambda)^2 - 1\right] - (2-\lambda)
  {% endequation %}
  {% equation %}
  = (2-\lambda)\left[(2-\lambda)^2 - 1 - 1\right]
  {% endequation %}
  {% equation %}
  = (2-\lambda)\left[(2-\lambda)^2 - 2\right]
  {% endequation %}


#### 4. Set this equal to zero and solve for $\lambda$:
  {% equation %}
  (2-\lambda)\left[(2-\lambda)^2 - 2\right] = 0
  {% endequation %}

So the eigenvalues are the solutions to {% equation_inline 2-\lambda = 0$ and $(2-\lambda)^2 - 2 = 0 %}.

- $2-\lambda = 0 \implies \lambda_1 = 2$
- $(2-\lambda)^2 - 2 = 0 \implies (2-\lambda)^2 = 2 \implies 2-\lambda = \pm\sqrt{2} \implies \lambda_2 = 2+\sqrt{2},\ \lambda_3 = 2-\sqrt{2}$

So the eigenvalues are $\boxed{2,\ 2+\sqrt{2},\ 2-\sqrt{2}}$.


### Obtain Eigenvectors
Let’s work this one out step by step. You’ve already correctly found the eigenvalues of

{% equation %}
A = \begin{bmatrix}
2 & 1 & 0 \\
1 & 2 & 1 \\
0 & 1 & 2
\end{bmatrix}
{% endequation %}

The eigenvalues are:

{% equation %}
\lambda_1 = 2, \quad \lambda_2 = 2+\sqrt{2}, \quad \lambda_3 = 2-\sqrt{2}.
{% endequation %}

Now let’s find the eigenvectors.

---

#### 1. General method

For each eigenvalue $\lambda$, solve

{% equation %}
(A - \lambda I) v = 0
{% endequation %}

for $v \neq 0$.

---

#### 2. Eigenvector for $\lambda_1 = 2$

{% equation %}
A - 2I =
\begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 0
\end{bmatrix}
{% endequation %}

So we need to solve:

{% equation %}
\begin{cases}
y = 0 \\
x+z = 0 \\
y = 0
\end{cases}
{% endequation %}

That gives $y=0$, $z=-x$.
So an eigenvector is

{% equation %}
v_1 = \begin{bmatrix} 1 \\ 0 \\ -1 \end{bmatrix}
{% endequation %}

---

#### 3. Eigenvector for $\lambda_2 = 2+\sqrt{2}$

{% equation %}
A - (2+\sqrt{2})I =
\begin{bmatrix}
-\sqrt{2} & 1 & 0 \\
1 & -\sqrt{2} & 1 \\
0 & 1 & -\sqrt{2}
\end{bmatrix}
{% endequation %}

System:

{% equation %}
\begin{cases}
-\sqrt{2}x + y = 0 \quad \Rightarrow \; y = \sqrt{2} x \\
x - \sqrt{2} y + z = 0 \\
y - \sqrt{2} z = 0
\end{cases}
{% endequation %}

From the first: $y = \sqrt{2}x$.
From the third: $z = y / \sqrt{2} = x$.
So eigenvector is

{% equation %}
v_2 = \begin{bmatrix} 1 \\ \sqrt{2} \\ 1 \end{bmatrix}
{% endequation %}

---

#### 4. Eigenvector for $\lambda_3 = 2-\sqrt{2}$

{% equation %}
A - (2-\sqrt{2})I =
\begin{bmatrix}
\sqrt{2} & 1 & 0 \\
1 & \sqrt{2} & 1 \\
0 & 1 & \sqrt{2}
\end{bmatrix}
{% endequation %}

System:

{% equation %}
\begin{cases}
\sqrt{2}x + y = 0 \quad \Rightarrow \; y = -\sqrt{2}x \\
x + \sqrt{2}y + z = 0 \\
y + \sqrt{2} z = 0
\end{cases}
{% endequation %}

From the first: $y = -\sqrt{2}x$.
From the third: $z = -y/\sqrt{2} = x$
So eigenvector is

{% equation %}
v_3 = \begin{bmatrix} 1 \\ -\sqrt{2} \\ 1 \end{bmatrix}
{% endequation %}

---

#### 5. Final result

The eigenpairs are:

* $\lambda_1 = 2 \;\;\Rightarrow v_1 = [1,0,-1]^T$
* $\lambda_2 = 2+\sqrt{2} \;\;\Rightarrow v_2 = [1,\sqrt{2},1]^T$
* $\lambda_3 = 2-\sqrt{2} \;\;\Rightarrow v_3 = [1,-\sqrt{2},1]^T$

---

✅ Notice: since $A$ is **symmetric**, the eigenvectors are mutually orthogonal (check dot products — they’re zero).

---















<!-- Av_i =lambda_i v_i
A*V=V*D, V=[[...],[v1,v2,...,vn],[...]].T
A*[v1,v2,...,vn]=[col.Av_1 col.Av_2 ... colAv_n]
V*V.T = I = V.T*V
V*D = [col.lambda_1*v_1 ... col.lambda_n*v_n]
A=V*V.T=V*D*V.T
A=V*D*V.T

example:
with Av=λv and |A-λI|=0
A = (3 1)
    (1 3)

Solve λ, given the values of λ solve (A-λ_1*I)*v_1=0

what happens when A is (1 4)
                       (3 2) -->
## Fitting a line (least squares)
E(l_1,P_i)= y_i-ax_i-b
E(P, l) = sumatory{n, i=i}(y_i-ax_i-b)^2
?what is the derivative of this formula for? with derivative of "a" and "b" we can make a system of equations?   

we can somehow extract the least square error with matrix operations
E(x) = ||Ax-b||^2 = <Ax-b, Ax-b> = (Ax-b).T*(Ax-b) = (x.T*A.T-b.T) (Ax-b)... = x.T*A.T*A*x-2x.T*A.T*b+b.T*b

we can now get the gradient of E -> ∇E=0
∇E = 2A.T*Ax-2A.T*b=0 -> A.T*Ax = A.T*b


## Analysis (curvatures)
## Simplification

- Lagrange multiplier & lagrange functions

  Lagrange multipliers and Lagrange functions are mathematical tools used to find the maximum or minimum of a function subject to one or more constraints. Instead of searching for extrema in the whole space, they allow us to optimize a function while ensuring that certain conditions (constraints) are always satisfied. The method introduces new variables (the multipliers) and combines the original function and the constraints into a single function, called the Lagrangian. By finding the critical points of this Lagrangian, we can solve constrained optimization problems efficiently.

  To simplify the process we can eve use matrix notation. You want to minimize a quadratic error function $E(n,d)$ with n = normal vector and d=distance from the origin of an hyperplane.

{% equation %}
  E(n,d)
{% endequation %}

  * n = vector to optimize (normal to plane).
  * d = scalar shift.

Given points 
$p_i$, the signed distance from a point to the plane is:
{% equation %}
  dist(p_i,n,d) = n \cdot p_i + d
{% endequation %}
If we want the best-fit plane (least squares), we minimize:
{% equation %}
  E(n,d) = \sum{(n \cdot p_i + d)^2}
{% endequation %}
That’s the quadratic error function.

If you try to minimize E(n,d) directly, the trivial solution is n=0,d=0, which is meaningless.

We need a constraint:
{% equation %}
  ||n||^2=1
{% endequation %}
to ensure the normal is a unit vector.

We now minimize:
{% equation %}
L(n,d,λ)=E(n,d)−λ(∥n∥^2−1)
{% endequation %}
* Here, λ is the Lagrange multiplier.
* Taking derivatives w.r.t. n,d,λ gives a system of equations.
* Solving it gives the optimal plane normal n and offset d.


The derivatives are:

* With respect to $n$:
{% equation %}
\frac{\partial L}{\partial n} = 2 C n + 2 d c - 2 \lambda n
{% endequation %}
  (where $C$ and $c$ are the covariance matrix and mean vector, depending on how $E(n,d)$ is written)

* With respect to $d$:
{% equation %}
\frac{\partial L}{\partial d} = 2 c^T n + 2 d N
{% endequation %}
  (where $N$ is the number of points)

* With respect to $\lambda$:
{% equation %}
\frac{\partial L}{\partial \lambda} = - (\|n\|^2 - 1)
{% endequation %}

Setting these derivatives to zero gives the system of equations whose solution yields the optimal $n$ and $d$.

<!-- 
the professor said "solutions to opt f(x) s.t g(x)=0
are the critical points of Lagrange"

It is doing partial derivative of lambda, x and y (not sure why)
We are given a function for a line 

l = ax + by + c = 0
but also an alternative version which can  also represent spheres. This is a bit confusing
h^t p + d = 0
||n|| = 1
d(p,l)=n^T p + d
E(l) = sum(n, i=1) d(p_i, l)^2 = sum (n^T p_1 + d)^2
We also can understand Error as 
E(n,d); OPT E(n,d) s.t. ||n||=1

We end up with n^T C n - lambda(n^T n -1) where C is a covariance matrix

This means 
derivative lagrange / derivative n = 2*C*n-2*lambda*n=0 which means C*n = lanbda*n
n will become Eigenvectors of C
lambda will become eigenvalues of C

E(n,d) = n^T * C * n = n^T * lambda * n = lambda*n^T*n = lambda

n eigenv of C with smallest eigenv lambda -->

## getting points
Having multiple point clouds knowing the camera position allows us to know the geometry of the photographed object

### Alignment (registration)

Given two point clouds, {% equation_inline P = \{P_i\} \text{ for } 1 \leq i \leq n %} and {% equation_inline Q = \{Q_i\} \text{ for } 1 \leq i \leq n %}, we seek a transformation—typically a rotation and translation—that aligns $Q$ to $P$.

The goal is to find a rotation matrix {% equation_inline R %} and a translation vector {% equation_inline t %} such that:

{% equation %}
P_i \approx R Q_i + t \quad \text{for } 1 \leq i \leq n
{% endequation %}

This process is known as point cloud registration or alignment.

### SVD (singular value decomposition)
{% equation_inline A \in \mathbb{R}^{n \times n} %}
Eigendecomposition:
{% equation %}
A = V D V^T
{% endequation %}
where
* {% equation_inline V %} contains the eigenvectors of {% equation_inline A %},
* {% equation_inline D %} is a diagonal matrix of eigenvalues,
* {% equation_inline V^T %} is the transpose of {% equation_inline V %}.

Singular Value Decomposition (SVD):
{% equation_inline A \in \mathbb{R}^{m \times n} %}
{% equation %}
A = U \Sigma V^T
{% endequation %}
where
* {% equation_inline U \in \mathbb{R}^{m \times m} %} is an orthogonal matrix,
* {% equation_inline V \in \mathbb{R}^{n \times n} %} is an orthogonal matrix,
* {% equation_inline \Sigma \in \mathbb{R}^{m \times n} %} is a diagonal matrix with singular values.

Orthogonal matrices ({% equation_inline U %} and {% equation_inline V %}) represent rotations or reflections.


{% equation %}
\Sigma = \operatorname{diag}(\alpha_1, \alpha_2, \ldots, \alpha_{\min(m,n)})
{% endequation %}
where each diagonal entry {% equation_inline \alpha_i %} is a singular value of {% equation_inline A %}, and {% equation_inline \alpha_i \geq 0 %}.


Given a linear system {% equation_inline Ax = b %}, where {% equation_inline A \in \mathbb{R}^{r \times n} %}, we can use the Singular Value Decomposition (SVD) to solve for {% equation_inline x %}. If {% equation_inline A = U \Sigma V^T %}, then:

{% equation %}
U \Sigma V^T x = b
{% endequation %}

Multiplying both sides by {% equation_inline U^T %}:

{% equation %}
\Sigma V^T x = U^T b
{% endequation %}

Let {% equation_inline y = V^T x %}, so:

{% equation %}
\Sigma y = U^T b
{% endequation %}

Solving for {% equation_inline y %}:

{% equation %}
y = \Sigma^{-1} U^T b
{% endequation %}

Then, recovering {% equation_inline x %}:

{% equation %}
x = V y = V \Sigma^{-1} U^T b
{% endequation %}

This expresses the solution to {% equation_inline Ax = b %} using SVD.


### Simpler Alignment
Given two point clouds:
- {% equation_inline P = \{P_i\} \text{ for } 1 \leq i \leq n %}
- {% equation_inline Q = \{q_i\} \text{ for } 1 \leq i \leq n %}

Assume:
- Both sets have the same number of points ($n$).
- Each $P_i$ corresponds to $q_i$.

We seek a rotation matrix {% equation_inline R \in \mathbb{R}^{3 \times 3} %} and a translation vector {% equation_inline t \in \mathbb{R}^3 %} such that:

{% equation %}
P_i \approx R q_i + t
{% endequation %}

The alignment error function is:

{% equation %}
E(R, t) = \sum_{i=1}^{n} \| P_i - R q_i - t \|^2
{% endequation %}

To solve for $t$:
- Compute the centroids of $P$ and $Q$:

{% equation %}
P' = \frac{1}{n} \sum_{i=1}^{n} P_i
{% endequation %}
{% equation %}
q' = \frac{1}{n} \sum_{i=1}^{n} q_i
{% endequation %}

- The optimal translation is:

{% equation %}
t = P' - R q'
{% endequation %}

So, the aligned points satisfy:

{% equation %}
P' = R q' + t
{% endequation %}

### Steps
1. Compute centroids {p', q'}
2. compute centroids adjusted
    (p'=p_i-p') q`= q_i-q'

3. S = QP.T = (q'1, q'2, ..., q'n) * ([p'1],[p'2],[...],[p'n])
4. S = U*covariancematrixsymbol*V.T
5. R=V*U.T
6. t = p'-R*q'

### Iterative closest point (ICP)
1. compute correspondence
  For every q_i fing closest p_k
2. Use simpler alignment (SVD)
   * Discard Points in P without correspondence
   * Replicate points in P multiply selected.
  From this points we get a P' that will be used temporarily
  Get R,t 
3. Apply R_it  [unknown symbol] -> Q q_i' <- Rq_1 + t
4. Repear from 1
  

### Stop criteria
1. norm(R-I)_f < epsilon where ||t||<e
   1. Correspondance constant
2. Maximum # iterations
   1. need initial rough alignment (there are better alternatives in the literature)

### Compute border point status
One of the posible problems is that point clouds are apart and the closest point for each other is a single point from P for all Q points. 

One solution could be to remove all overlapint points. The single point in P that corresponds to multiple Q points is called border point on P.
1. how do we detect border points and overlapped points?
  We will recognise border points by checking the size of the angle that does not posses any point.

For every point P_i:
  1. compute K-NN(Nearest Neighbours) of P_i -> {Pij} 1<=j<=k
  2. PCA -> Eigenvectors(v1,v2,v3==normal) + P_i == local frame (local coord system)
  3. (xij, yij, zij) = ((Pij -Pi)*V1, (Pij-pi)*V2, (Pij-pi)*V3)
  4. Discard Z'ij 
  5. Compute direction by taking the angle alpha_{ij}=atan2(y'_ij, x'ij)
  6. sort alpha_{ij}
  7. Differenes of adjacent pairs
     1. we call them deltaSymbol alpha_{ij}
  8. max(delta alpha_{ij}) >=B <=> P_i border point

(parameters -> K,B(Beta))

### Multiple Scans
Overlap -> 1st step of ICP -> compute correspondence
Overlap == Percentage of discarded points of Q
1. Select 2 unaligned clouds
2. Perform ICP on them result(successfull | fail)
   1. successfull == ||R-I||_f means ||t||=0 -> fuse the 2 clouds
   2. fail == success

### Solving linear systems
Ax=b, A belongs R^{nxn}, xi b belongs to R^n
Apply solvers
* Direct Performs op's to solve exactly (except machine precision)
* Iterative start with x0 estimation and then refine estimation.

### Properties and requirements to how we want to resolve the system
* Required performance
* Reuqired accuracy
* Limits for storage
* Problem: Dimensional n (A belongs to R^{nxn}), n > 10000 -> LARGE
* Sparsity of A == Percentage of O's in A (sparse|dense)
* symetry of A
* Positive definiteness of A
  * InvertedASymbol=/0 -> x.T * A * x > O <=> A is positive definite

### decompositions
* Direct LU decomposition -> A=LU 
  * L== lower triangular
  * U == Upper triangular
  * Ax=b -> LUx = b -> L_z = b, V_x=z
* QR decomposition A=QR -> R==Uper triangle, Q == Orthogonal
  * Ax=b -> QRx=b <=> Rx=z
  * z = Q.T*b == Qz=b 
* Cholesky
  * A is symetric and positive definite
  * A = LL.T / L==lower triangular

#### interactive
* Conjugate gradient (CG) A symetric and positive definite
* Biconjugate gradient (Bi-CG) 


<!-- ## Smoothing (remove noise)
## Remeshing
## Parametrization
## Model repair
## Editing and reformation
## Synthesis
## Symetry detection -->

# Session 3
Many reconstruction algorithms reduce (or contain subproblems that reduce) to linear systems. Choosing the right solver is crucial for performance and memory.

in general is more complex than this, but this works for most problems to choose a solver
1. Decide the size of the system. Depends a little bit on the system. size is  a convention made by people, not a mathematical solution
   1. large: means matrix is sparse. 
      1. A is symetric and positive matrix, then we can use CG
      2. if not, Bi-CG
      3. if it is large and dense: 
         1. Transform into sparse
         2. Specific solver for the problem we are going to solve.
         3. Brute force approach: highly parallel solver + hardware
   2. small: A symm+pos.det->cholesky
      1. other options QR & LU decomp.

## reconstruction
Reconstruction
      {p1 belongs to real number R^3}1<=i<=n and {n_i belongs to real number R^3}1<=i<=n. -> reconstructor -> surface (triangle mesh). For all points that belong to the surface face, q belongs to U of size n, Sphere(pi,epsilon) =>Q={q_i} p-dense w.r.t. S

      P={p_i}, pi=q_i+e_i, q_i belongs to S

      ||e_i||<& => P is delta-noisy
      If P is delta-noisy w.r.t. Q set inside S set, Q is delta-dense w.r.t S

      Therefore P is epsilon-dense & delta-noisy w.r.t. S

What we usually do is:
  - Ponit cloud -> reconstruct -> implicit function == f D->R where D intersects R^3 -> isosurface extraction -> triangle mesh
    - f(p) = 0  -> P is S
    - f(p)>0 -> P outside S
    - f(p)<0 -> p inside S

## algorithms for reconstructions
- A_inverted_symbol _i where 1<=i<=n f(p_i)=0 -> interpolatory

- A_inverted_symbol _i where 1<=i<=n f(p_i) close_to 0 -> approximative

* Hoppe et al. qa
  This is NOT a smooth function

  f(p)
    i <- Index of p_i closest to p.
    z <- p-((p-p_i)*n_i)*n_i
    if distance (z_i,p_i)<= delta+f then f(p) <-(p-p_i)*n_i
    else f(p) <- undefined

  * problems:
    * having f(p) undefined means we are going to get holes
    * Discontinuities: all this algorithm is interpolatory, these methods are usually not desired
* Moving least squares:
  This is a smooth function. Approximative algorithm. No holes, no quality for empty space.

  f_i(p) = (p-p_i)*n_i == bump function

  f(p)=sum(n, i=1) W_i(p) f_i(p) / sum(n, i=1) W_i(p)

  for the weight function W_i(p) we can choose whatever we see fit. W_i(p) = exp(-||p-p_i||^2 / euro_symbol^2)/A_i where A_i is #points closest to p than euro_symbol
    Asks for p to be euro_symbol-dense

* Radial Basis Functions
  Prior information, usually smooth prior
  The sumation of the "n" bump functions pases through some points but not necessarily over others. f(p)=c_1 * f_1(p) + c_2 * f_2(p) - c_3 f_3(p)

  f(p_1)=f(p_2)=f(p_3) = 0


  (p_i,v_i) with   1<=i<=n
    f(p_i)=v_i
    f(p)=sum(n,i=1) C_i f_i(p)
    f_i(p)=phi(||p-p_i||)
    phi(r)=exp(-r^2/2c)

    f(p_i)=v_i=sum() c_i phi(||p-p_i||)

      n equations 
      c_i = n variables
    
    A*c = v | A = [[phi(||p_1-p_1||), ..., phi(||p_n-p_n||)] [...] [phi(||p_1-p_n) ... phi(||p_n-p_n)]]

  * problems
    * As we have defined v=0=> C=0
      The solution to this:
      We add syntetic points. For every point p_i we generate a p_i^+ equal to p_i+d*n_i

      f(p_i^+) = d
      f(p_i^-) = -d

      now the problem is that we increase the size of the point sets 3 times on each direction, the result is 9 times bigger.

      A*c=v becomes unstable as n increases.
        Regularization A' = A+lambda*I
        (A+lambda*I)*c=v

      Interpolatory => Approximate

    * In case we have a large point set we cant triplicate the size problem.
      * compact support
        * phi(r) = {exp(-r^2/2c^2),      r<3c}
                   {0,              otherwise} 

* Poisson Surface Reconstruction
  It has a set of points and normals and tries to solve the indicator function where 0 is outside and 1 is inside

* SSD



# Session 4
We have completed the basic reconstruction pipeline.
We have the normals and triangles.


Now we can compute the Curvature (curvature is refered as K).
- How much something bends.
- Interesting properties:
  - Translation, Rotation invariant
  - local surface characteristics => Geometric signature
  - First definition
    - Deviation from straighttaps?straightlines
    - Osculating circle
      - "oscular"->kiss
      - we create osculating circles R == radious of curvature K=1/R
    - Osculating circle center as intersection of perpendiculars
    - Newton realised that there was a connection between f''(x)<->K. He's own formula gave an interesting result whenever we compute K at (0,0) and the derivative of f(0)=0. Then the curvature of K(0,0) is equal to f''(0)
    - Others realised that Curvature == Rate of turning
    - K= derivative gamma / derivative S
  - So we have lots of definitions for a curve. But this is for curves, we now need it for surfaces
- Surfaces
  - curve == π in S -> K_{P,π}  
  - S smooth (continous second derivative f''()) then K at p for S cam be defined. K_{min}, K_{max} == Min, Max K's
  - V_{min}, V_{max} == Directions of K's
  - This are principal Curvatures of S at P
  - In a cylinder we should have a curvature where K_min=0 and K_max=1/R

  - This is an example i dont really get: 
    - y=ax^2; K=f''(x)=a>0; normal_direction = -y; This is convex
    - f(x) == -ax^2; k=f''(x)=-a<0;normal_direction = -y; ; Concave
  - I can have diferent combination of values for K_min and K_max; Kmin>0,Kmax>0 convex; Kmin<0, kmax<0 concave; kmin=0, kmax>0 parabolic convex; Kmin<0, kmax=0 Parabolic concave; Kmin <0, Kmax >0 Saddle. 
  
  - Computation of K in 2D: first move the curve to cross (0,0) in a point that its second derivative is also (0,0).We imagine that for a second curve fitting a segment of the original curve is just a collection of points. f(x) -> compute least squares; f(x) = ax^2+bx+c; compute K= f''(0) = 2a; f'(x)=2ax+b

That was the idea for 2D. We are going to have surfaces. We will assume surfaces are poits or triangle meshes so we will look for neighbours. In many cases we will do it restricting the neighbour points along the geometry.

Following same steps as in 2D we must transform the geometry to cross the origin.
What is the second derivative on a surface and how do we calculate it?
- W(u,v) = au^2+buv+cv^2+du+ev + f
- y = f(x)=ax^2+bx+c
Something that uses multivalua calculus, there is an analogous quantity to the second derivative.
- f(x)->f'(x) -> f''(x)
- w(u,v) -> graphSymbol w = -> hessian value Hw=([integral²w/integral u^2,integral²w/integral u integral v ], [integral²w/integral vintegral u, integral^2w(integral y^2)]) -> Eigen decomposition -> lambda1 = kmin, lambda2 = Kmax; v1 , v2
  - Input: Pi, {p_ij} 1<=j<=K K-NN of Pi
  - Output: p_i Kmin, p_i Kmax
            p_i Vmin, p_i Vmax
  1- LOCAL FRAME -> V= -normal
                    U= OX cross V (if they are parallel we can use other arbitrary vector | u=OYxV|,|U=OZxV|)
                    W=U cross X

  2- (U_ij, V_ij, W_ij) = ((p_ij-p_i)*U,(p_ij-pi)*V,(p_ij-pi)*W)

    W = W(u,v) = au^2+buv+cv^2+du+ev+f = q^T S where Q= (u², uv, v²,u,v,1)^T and S=(a,b,c,d,e,f)^T
    E(S) = sum(K,j=1)(W(u_ij,v_ij)-Wij)² = sum()(1_ij^T S- Wij)² = sum()(q^T_ij*S * q^T_ij*S)-2*Sum(W_ij*q^T_ij)*S + sum(W_ij^2) = ... = S^T*(Sum(q_ij q_ij^T)) S - 2 S^t*(sum(w_ij q_ij)) + sum(w_ij²)

    MIN E(S) <=> delta E = 0 -> delta E = 2*(sum (q_ij, q_ij^T))*S -2(sum(W_ij, q_ij))=0

    (sum(q_ij q_ij^T))*S = sum w_ij  q_ij

    A*S = b
    A belongs to R^{6x6}, b belongs to R⁶

    q_ij*q_ij^T = [...].T * [....]

    derivate W / derivate u = 2au+bv+d
    derivative²w/derivative u = 2a
    derivative² w / derivative v derivative u = derivative² w / derivative u derivative v = b
    derivative² w/ derivative v² = 2C
    Hw = (2a,b)(b, 2c) -> eigen decomposition kmin,kmax,vmin,vmax

Gaussian Curvature
- K_g = kmin*kmax=k
  - Mean curvature -> K_m = (kmin+kmax)/2 = H
  - K=0 ---> H=0 ->PLANAR
        ---> H<0 -> Parab Concave
        ---> H>0 -> Parab convex

  - Kmax,Kmmin = H+- sqrt(H²-K)
    ---> K>0 
          ---> H>0 Convex
          ---> H<0 concave
    ---> K<0 SADDLE 

  
# Session 5

## Smoothing (denoising)

### Laplacian smoothing
"Umbrella" operator. 

L(v_i)==Discrete Laplacian of v_i

### Mesh (triangle)
This smethods work for any type of mesh
M=(G,P)
G=Mesh Graph: how vertex are connected
P=Geometry || P belongs to R^{nx3} where n is the number of vertices

G=(V,E)
V={i | 1<=i<=n}
E is inside VxV

### 1-ring neighbourhood
N(i)={j|(i,j) belong to E}

L(v_i)=1/|N(i)| * sum(j belongs to N(i), v_j-v_i)

### Laplacian iterative operator
Update rule == Vi' <- v_i+lambda*L(v_i) where lambda belongs (0,1]

Choosing lambda -> compromise
* Large lambda -> larger steps, faster smoother
* small lambdas -> Avoid vibration

If we look at the behavior, if we have a vertex v_i with normal_i, for a convex vertex we lose area/volume.
On concave vertices we will gain area/volume

In closed shapes, convexity > Concavity
Laplacian == Net lost volume


There are other ways to do the update method.
- Biplacian: takes one step forward and one backward
    vi' = vi+lambda * L(vi)
    vi'' <- vi' - lambda * L(vi')
    
    This method loses less volume.

- Taubin's lambda-mu
    this guy Modified the biplacian 
    vi' <- vi + lambda * L(vi)
    vi'' <- vi' + mu*L(vi')

    1/lambda+1/mu =~ 0.1 -> 10mu +10lambda = lambda mu -> 10mu - 10lambda = -10lambda

    mu = -10lambda/10-lambda

We can also implement matrix form. This will open the door to different smoothing approaches
L=M^-1 * C

C is known as the week laplacian

M is known as the mass matrix, takes into account how many neighbours each vertex has

L laplacian matrix

         |
(C)_ij = |W_ij                            , i != j where (i,j) belong to E
         |-sum(k belongs to N(i), W_i*k)  , i=j 
         | 0                              , otherwise
         |

W_ij = 1 -> Uniform Laplacian

Cotagent laplacian tryes to preserve the distribution of the sampling

W_ij = 1/2 * (cotangent alpha_ij + cotangent Beta_ij )

This does not take into account that some vertex take into account more curvature and others less. 

now we should calculate the mass matrix mentioned before:
- mass matrix for the uniform laplacian
    (M)_ij = ||N(i)|         , i=j
             |  0            , otherwise
- Cotangent
    (M)_ij = | A_i           , i=j
             | 0             , otherwise

    A_i==1/3 sum(Areas of adjancent triangles to v1)


Once we have the matrix definition of L=M^-1*C the update method is simpler
P'=P+lambda L P'
P,P' belongs to R^{nx3}
L belongs to R^{nxm}
lambda belongs to (0,1]

I*P+lambda L P = (I + lambda * L) * P

P' = P+lambda L P 
p'' = p' - lambda L P'
p'' = (P + lambda L P )- lambda L (P + lambda L P) = 
= P+ lambda L P - lambda L P - lambda^2 L^2 P =
= (I-lambda^2 L^2) P 

### Global smoothing approach
The main idea is that the laplacian times P is computing the update vector. FOr each of the points the vector is larger if they are further from the neighbours. 

L*P' =~ 0 -> there is a simple solution -> P'=0; we dont want to turn everithing to 0

keep the update vectors as close to 0 as posible. We will assume that the M 

- Constraint laplacian:
    n first v's -> smooth {v_i | 1 <= i <= n }
    others -> constrain {v_i | m < i <= n }

    1<i <=n -> L(v'_i) = 0
    n<i<=n -> v_i ' = v_i
    
    All this is trying to be interpolative.

    L1 == First m rows of L

    A*P'=B , we can solve this with some matrix problem solver 

We want to create a matrix that contains: 
| L |       | 0 |
|---| P' =  |---|
| I |       | P |

This has 2n rows and n columns

(L^T | I) = ( L ) * p' = (L^t|I)*( 0 )
            (---)                (---)
            ( I )                ( P )  

Weight smoothing vs constraints
(lambda L / I ) P' = (O/P)
mu*I*P'=mu*P


P smooth = (I+Lambda*L)^k1 * P; k1 iterations of laplacian
P low    = (I+lambda*L)^k2 * P k2>k1

P' = Plow + mu (Psmooth- Plow)
mu = 1 -> P'=Psmooth
mu < 1  -> attenuating high frequency details
mu > 1 -> Exaggeratig high frequency details


# Session 6
Mesh parametrization

Attaching properties to a surface, how do we do that?
Unfold the mesh, we generate distortion wit few exceptions (geometrical objects like cube, cylinder)

meshes are a subset of R³. We get [f_x(u,v),f_y(u,v),f_z(u,v)]

what does derivate of f respect to u and derivate of f respect to v represent?

CAD: f's are polynomials or rsteral functions


Output of a mesh parametrization algorithm
It should give the coordinates of each vertex or "Corner". If you give coordinates of a cube for example, vertices have to be flattened out on the domain so each point is inside an image. It is complex to preserve continuity. That is why we create discontinuityes and assign corners to the UV map.

Bijectivity
- global bijectivity
- local bijectivity


Topological equivalence
f continuous:
    S_i virtually equal S_b

    When homeormophism, continuous | continuous inverse | (injection)


Applications : 
- Mesh morphing
- Detail transfer
- Mesh completion

# Session 7 - Geometry Processing: Distortion, Isometries & Parametrization

## 1. Introduction — What is This Lecture About?

This lecture focuses on **measuring and controlling distortion** when mapping between 3D surfaces and 2D domains.

In general, in **geometry processing**, we often have a 3D mesh (a surface embedded in {% equation_inline R^3 %}) that we want to **“unfold” or parametrize** onto a 2D plane {% equation_inline (R^2) %}, while preserving certain geometric properties — like angles, areas, or lengths.

So in this lecture:

* We start from a **2D parameter domain** {% equation_inline (u, v) \in \mathbb{R}^2 %},
* and a **mapping function** {% equation_inline f(u, v) \rightarrow \mathbb{R}^3 %} that defines a 3D surface.

The goal is to understand **how the local geometry changes** under this mapping — that is, how much stretching, shearing, or compression occurs when going from 2D to 3D (or vice versa, when flattening a 3D surface to 2D).

{% alert warning %}
This is meant o be applyed in objects that are topologically equivalent to a disc.

### The Goal of This Lecture (Session 7)

**how to analyze and classify mappings between surfaces**, so that you can later:

* **Decide what kind of parameterization** (flattening or texture mapping) is appropriate for your surface {% equation_inline S %},
* **Quantify distortion** (angle, area, or length distortion),
* **Choose the right mathematical tool** (isometric, conformal, or area-preserving mapping).

---

You’re taking a **mapping function**:
{% equation %}
f : (u,v) \in \mathbb{R}^2 ;\longrightarrow; \mathbf{x} \in \mathbb{R}^3
{% endequation %}
and, by examining its **Jacobian** {% equation_inline J %}, understand what kind of geometric distortion it introduces.

- Step 1: Compute the **Jacobian** {% equation_inline J %}
- Step 2: Compute the **First Fundamental Form**
- Step 3: Use **SVD decomposition** of {% equation_inline J %}
- Step 4: Interpret {% equation_inline \alpha_1, \alpha_2 %}

---


### What That Means Conceptually

You’re building the mathematical **intuition and tools** to look at a surface and say:

> “If I want to flatten this surface, what’s the best possible mapping I can use —
> one that keeps lengths, angles, or areas as close as possible to the original?”

So yes — the ultimate purpose is to **detect or choose** the correct type of transformation for your surface ( S ), based on:

* Its **geometry** (developable or not),
* Its **topology** (disk-like, sphere-like, with holes),
* And what property you care about preserving (angles, areas, or lengths).
{% endalert %}

---

## 2. Continuous Mapping and the Jacobian

We describe the local behavior of the mapping {% equation_inline f(u, v) %} using its **Jacobian matrix**, {% equation_inline J %}:

{% equation %}
J =
\begin{bmatrix}
\frac{\partial f_x}{\partial u} & \frac{\partial f_x}{\partial v} \\
\frac{\partial f_y}{\partial u} & \frac{\partial f_y}{\partial v} \\
\frac{\partial f_z}{\partial u} & \frac{\partial f_z}{\partial v}
\end{bmatrix}
{% endequation %}

This matrix describes how a small differential element in parameter space {% equation_inline (du, dv) %} transforms into 3D space. 
{% alert secondary %}
the matrix {% equation_inline J %} (the Jacobian of the mapping) is the gradient of the mapping function 
{% equation_inline f(u,v) %} — but expressed in matrix form.
It represents how the position in 3D changes with respect to small changes in 2D parameters.
{% endalert %}

The **metric tensor** {% equation_inline I %} (also called the **first fundamental form**) captures how lengths and angles are distorted:

{% equation %}
I = J^T J =
\begin{bmatrix}
E & F \\
F & G
\end{bmatrix}
{% endequation %}

where
- {% equation_inline E = f_u \cdot f_u %}
- {% equation_inline F = f_u \cdot f_v %}
- {% equation_inline G = f_v \cdot f_v %}

This tells us how the surface stretches or bends locally.

{% alert secondary %}
E, F, and G don’t each correspond directly to one specific type of distortion (angle or area) — instead, they jointly encode all distortions.

- E and G: describe stretching along coordinate directions
- F: describes angular (shear) distortion
- EG−F²: describes area distortion
{% endalert %}

---

## 3. Singular Value Decomposition (SVD) of the Jacobian

To analyze distortion, we apply **Singular Value Decomposition (SVD)** to {% equation_inline J %}:

{% equation %}
J = U \cdot \Sigma \cdot V^T
{% endequation %}

where:

*  {% equation_inline U %} and  {% equation_inline V %} are orthogonal matrices representing rotations,
*  {% equation_inline  \Sigma = \text{diag}(\sigma_1, \sigma_2) %} contains **singular values** (positive real numbers).

The singular values {% equation_inline \sigma_1, \sigma_2 %} describe how much the surface stretches along the principal directions.

We can define:
{% equation %}
\alpha_1 = \sigma_1, \quad \alpha_2 = \sigma_2
{% endequation %}
and they represent **principal stretches**.

### Special Cases

| Type of Transformation  | Conditions on α₁, α₂     | Description                                                       |
| ----------------------- | ------------------------ | ----------------------------------------------------------------- |
| **Isometry**            | α₁ = α₂ = 1              | Perfect preservation of lengths and angles (rigid motion)         |
| **Conformal map**       | α₁ = α₂                  | Angle-preserving, but may change area uniformly                   |
| **Area-preserving map** | α₁ * α₂ = 1              | Keeps areas constant, but angles may distort                      |
| **Developable surface** | Gaussian curvature K = 0 | Surface can be flattened without stretching (e.g. cylinder, cone) |


{% figure size="1" cols="1" caption="Type of transformations example" %}
/assets/Posts/GPR/deformation/deformation.gif
{% endfigure %}
---

## 4. Isometries and Developable Surfaces

### Isometry

If both stretches are 1, the mapping preserves both **lengths and angles**.
Isometries correspond to **rigid motions** — translations, rotations, and reflections.

### Developable Surfaces

Developable surfaces are 3D surfaces that can be unfolded to a plane **without distortion** (isometric flattening).
They satisfy **Gaussian curvature {% equation_inline K = 0 %}**.

Examples:

* Cylinder (without caps)
* Cone

Non-developable surfaces (like the saddle) have negative Gaussian curvature (K < 0).
Even though they contain straight lines — in fact, two families of them intersecting at each point — the surface cannot be flattened isometrically because it bends in opposite directions along those lines.

---

## 5. Discrete Case — Mesh Parametrization

When working with **triangle meshes**, we want to find a 2D parametrization of the vertices that minimizes distortion.

Given a triangle with vertices {% equation_inline P, Q, R %} in 3D and their corresponding 2D coordinates {% equation_inline P', Q', R' %}, the local transformation {% equation_inline M %} that maps 3D edges to 2D edges can be written as:

{% equation %}
M [R - P, Q - P] = [R' - P', Q' - P']
{% endequation %}

or equivalently:

{% equation %}
M = M_2 M_1^{-1}
{% endequation %}
where
{% equation %}
M_1 = [Q - P, R - P], \quad M_2 = [Q' - P', R' - P']
{% endequation %}

We can again use {% equation_inline M^T M %} to study local distortion — its eigenvalues correspond to the squares of the singular values {% equation_inline \sigma_1^2, \sigma_2^2 %}.

---

## 6. Conformal Maps

A **conformal map** preserves angles but not necessarily areas.

Mathematically, conformality requires that the **Jacobian satisfies**:
{% equation %}
f_v = f_u \times n
{% endequation %}
where {% equation_inline n %} is a rotation by 90° in the tangent plane.

This implies:
{% equation %}
\alpha_1 = \alpha_2
{% endequation %}

### Examples of Conformal Transformations:

* Translations
* Rotations
* Uniform (isotropic) scalings
* Reflections

---

## 7. Area-Preserving Transformations

For an **area-preserving** mapping:
{% equation %}
\det(J) = \alpha_1 \alpha_2 = 1
{% endequation %}

That means the **local area** of the parameter domain and the corresponding patch on the surface are equal.

---

## 8. Balancing Distortions — Energy Minimization

In practice, exact isometries or perfect conformal/area-preserving maps are rare.
Instead, we define **energy functions** to measure distortion and minimize them.

Example energy functions:

* **Conformal distortion energy:**

{% equation %}
E_c(\alpha_1, \alpha_2) = \mu_c (\alpha_1 - \alpha_2)^2
{% endequation %}

→ minimized when α₁ ≈ α₂.

* **Area distortion energy:**
{% equation %}
  E_a(\alpha_1, \alpha_2) = \mu_a \left( \frac{\alpha_1}{\alpha_2} + \frac{\alpha_2}{\alpha_1} \right)
{% endequation %}
→ minimized when α₁ * α₂ ≈ 1.

Different parametrization methods optimize different combinations of these.

---

## 9. Historical Approach — Tutte’s Embedding

**Tutte (1963)** proposed one of the first parametrization algorithms:

> “How to draw a graph”

* Works for **planar graphs**.
* Fixes boundary vertices to a convex polygon.
* Interior vertices are placed at the **barycenter** of their neighbors, minimizing a “spring energy”.
* Guarantees **no intersections**, but produces **poor parametrizations** (large distortions).

Later methods improved Tutte’s approach by adjusting edge weights or using curvature-aware Laplacians.

---

## 10. Mean Value Coordinates (Floater, 2003)

To improve upon simple Laplacian weights, **Mean Value Coordinates (MVC)** are used.

Given a vertex {% equation_inline v_0 %} connected to neighbors {% equation_inline v_i %}:
{% equation %}
w_i = \frac{\tan(\alpha_{i-1}/2) + \tan(\alpha_i/2)}{||v_i - v_0||}
{% endequation %}
and normalized as:
{% equation %}
\sigma_i = \frac{w_i}{\sum_j w_j}
{% endequation %}

This gives:
{% equation %}
v_0 = \sum_i \sigma_i v_i, \quad \sum_i \sigma_i = 1
{% endequation %}

MVC preserves **angle properties** better and provides **stable parametrizations** even for non-convex boundaries.

---

## 11. Summary Table

| Property                | Condition   | Preserves             | Examples                     |
| ----------------------- | ----------- | --------------------- | ---------------------------- |
| **Isometric**           | α₁ = α₂ = 1 | Lengths, Angles, Area | Rigid motion                 |
| **Conformal**           | α₁ = α₂     | Angles only           | Rotations, uniform scaling   |
| **Area-preserving**     | α₁ * α₂ = 1 | Area only             | Shear or anisotropic scaling |
| **Developable surface** | K = 0       | Locally flattenable   | Cylinder, Cone               |

---

## 12. Key Takeaways

* The **Jacobian** and its **SVD** describe how a 2D patch is distorted when mapped to 3D.
* **Isometries** are ideal but rarely achievable for curved surfaces.
* **Conformal** and **area-preserving** mappings are relaxations used in practice.
* Parametrization algorithms (like Tutte or Floater) try to **minimize distortion energies** for meshes.
* **Developable surfaces** {% equation_inline K = 0 %} can be unfolded without stretching — most real surfaces are not.

---


# Session 8
- Distorsion singular values of A.
- Isometries, angle preserving (conformal)
  - cueo preserving (authalic) ?
- which surfaces admit isometric R
  - K==0 <-> developable surfaces
- Mix and match prop. (energy opt)
- MVC

---

- In depth analysis of planar param.
  - optimization goal: 
  - by alpha cordin ?
  - quality guarantees
  - cost

Last day we mentioned that Tuttes result is connected to the quality guarantees because a convex convination always produces bijections... (i dont actually know what he is refering to)
{% equation %}
M = (V,E,F) -> f(p_i) = U_i belonging to D in R²
{% endequation %}

V are the {P_i}
E are the edges {(i,j)}
D can be any area like a circle or square or arbitrary surface.

We have a boundary {1,...,n}=interior and {n+1,...,N}=derivave M
N_i = {j|(i,j) in E}


U_i for i in Interior points = 1/(sum w_i*j where i belongs to N_i) * (sum w_i*j*U_j where j belongs to N_i)

<!-- Here he lost me a bit: -->
U is the surface composed of (M_i, N_i). we can create matrix A * [M_1, M_2...].T that is the same as B [U_{n},...,U_N]

a_{ij} is:
- sum w_{ik} if n=i
- -w_{ij} if i != j and (i,j) belongs to E
- 0 otherwise

A will only depend on W_{ij}. B will be W_{ij} and is multiplying fixed delta schemes.

The selection of points in boundary and what can be expected. We have the points in R³, i draw a curve that is a polygon and i can compute the lenght L of the hole poligon as the sum || p_i-p_{something} ||^{alpha} where i is {N_1..., N}

Now for the other points i choose an image for the next point and i compute the distance to the boundary
d_j = sum ||p_i-p_{something}||^{alpha}

d_j/L belongs to [0,1]

d_i/L * P; (P is perimeter)

--- 
<!-- Second part of session 8 -->
Lets draw points  and we add a spring between the points. We can calculate how much they are being pressed or separated with E=1/l * k * s². We name this srings k

With this information we get that 
E = 1/2 (sum 1/2 * k_{ij} ||p_i - p_j||²) = 1/4 (sum k_{ij*(pi_pj).T*(pi-pj)})

delta E / delta M_i = 1/4 (sum k_{ij} 2*(p_i-p_j)) ? not finished, i assume it is summing all the springs or something.


Linear reoriducibility: Tutte only cares about geometry (the connection of points) moving the non boundary points. This does not have reproducibility. 

This means that A scheme will have reproducibility only if it represents the baricenter coordinates.

Baricenter coordinated for poligons
- cotangent weights (harmonic params)
- w_ij = (ctan(alpha) + cotan(beta)) / 2
- cotangent weights can be negative, diferently to tutte, we can not assure to have a 1 to 1 parametrization.

Waclispaces coordinates
- w_ij = (cot(alpha) + cot(beta)) / || pi-pj ||

eigendecomposition, less square, laplacian.

<!-- For some reasom, this is meant to be strongly relationated with laplacian  -->


# Session 9
<!-- Last day we saw Analysis of scheme and based affine combinations. We also explored the NN spring model.

- Linear reproduction
<-> scheme == baryanlic coro?
p_i = sum(j in d(i), w_ij * p_i) for triangle uniq...

Now we explore:
* Cotangent weights:
  * harmonic weights
  * + Matrix is non uniform
  * + small angle dist
  * - negative weights
* wachpress weights
  * in two connected triangles on an edge:
    w_j = (cot \alpha + cot \beta / ||p_i-p_j||) 
    where:
      - p, p_j, d, p_{j+1} are angles
    
    w_j = B_j / (A_{i-1} * A_j)
We caclulate the varicentre to recalculate the point b?

B_j*P = A_j * p_{i-1} = (A_{i-1}+A_j-B_i)*P_j + A_{i-1}
= A_j (P_{j-1} - P_j) - A_{j-1}(P_j-P_{j+1}) + B_j *\phi_i


sum(j, B_i / A_j*A_{j-1})(p-p_j)=0

We still need to discuss somethings like three boundary points to make use of this section.

- MVC: floater
Do we have some free boundary?


- **Most isometric Paom**
  - start with a guess (with good properties) usually fixed derivative.
  - Iterativelly improve past of "worst" point  (int ac derivative) 



NEXT TOPIC: REMESHING
WE need a metric that tells us what is better "mesh"
- types of primitives
  - triangle, square, hexagonal...
-->

“Last day we looked at linear schemes for parameterization, affine combinations, and the ‘spring model’ (Tutte).

The important condition from last time is:”

### **Linear reproduction**

We want a coordinate scheme that satisfies:

{% equation id="linear-reproduction" %}
p_i = \sum_{j \in N(i)} w_{ij} \, p_j
{% endequation %}

for **any** *affine* function {% equation_inline p %}.
This means the weights reproduce linear functions exactly.
This is the definition of **barycentric coordinates** on a mesh.

> “If your weights reproduce linear functions, the scheme is affine, and therefore suitable for mesh parameterization.”

{% alert primary %}
**Reminder**: What are weights?
Weights {% equation_inline w_{ij} %} are scalars assigned to each edge {% equation_inline (i,j) %} used to express an interior vertex (or point) as an affine/barycentric combination of its neighbors:
{% equation_inline p_i = \sum_{j \in N(i)} \sigma_{ij} \, p_j %}, with {% equation_inline \sigma_{ij} = w_{ij} / \sum_k w_{ik} %} (normalization)
{% endalert %}

---

## **2. Today: Three families of barycentric weights**

Today’s lecture surveys **three classic weight constructions** used in mesh parametrization:

1. **Cotangent weights** (a.k.a. harmonic weights)
2. **Wachspress coordinates**
3. **Mean Value Coordinates (MVC)** by Floater

For each:

* why it's good
* why it's bad
* where it comes from
* how it affects parameterization quality

---

## **3. Cotangent weights**

These arise from discrete differential geometry.
They are connected to the Laplace–Beltrami operator.

### **Definition (for a triangulated mesh)**

For an interior edge {% equation_inline i \leftrightarrow j %}:

{% equation id="cotangent-weights" %}
w_{ij} = \frac{1}{2} \left( \cot \alpha_{ij} + \cot \beta_{ij} \right)
{% endequation %}

where {% equation_inline \alpha_{ij} %} and {% equation_inline \beta_{ij} %} are the angles opposite the edge {% equation_inline (i,j) %} in the two adjacent triangles.

These are the weights used in **harmonic maps** and **discrete Laplacian**.

### **Properties**

**+ Very good angle behavior**
They minimize Dirichlet energy and give low angle distortion.

**+ Produce very smooth parameterizations**

**– Can produce negative weights**
If a triangle has an obtuse angle, the cotangent of an obtuse angle is negative.
Negative weights break injectivity → possible fold-overs in parameterization.

**– Matrix is non-uniform**
The Laplacian matrix with cot weights is symmetric but not uniform, and not always diagonally dominant.

> “Cotangent weights are great, but dangerous. They are the gold standard for geometry processing, but you have to watch the signs.”

---

## **4. Wachspress coordinates**

These originate from projective geometry.
They are generalized barycentric coordinates on convex polygons.

Consider a convex polygon with vertices {% equation_inline p_1, p_2, \ldots, p_n %}.
For each vertex {% equation_inline p_j %}, the **Wachspress weight** at interior point {% equation_inline p %} is:

{% equation id="wachspress-weights" %}
w_j(p) = \frac{B_j(p)}{A_{j-1}(p) \, A_j(p)}
{% endequation %}

Where:

* {% equation_inline A_j %} = signed area of triangle {% equation_inline (p, p_j, p_{j+1}) %}
* {% equation_inline B_j %} = area of the quadrilateral formed by edges meeting at vertex {% equation_inline j %}

These areas appear in formula:

{% equation id="wachspress-simplified" %}
w_j = \frac{B_j}{A_{j-1}A_j}
{% endequation %}

This matches the Wachspress construction.

### **Properties**

**+ Always positive (for convex polygons)**
No negative weights → preserves injectivity.

**+ Coordinate functions are rational**
Smooth inside, but more complex.

**– Only guaranteed to work on convex polygons**
Not useful on arbitrary boundary shapes.

---

## **5. Where the “baricenter equation”  comes from**

This is the professor deriving the **linear precision condition**:

{% equation id="linear-precision" %}
\sum_j w_j(p) \, p_j = p
{% endequation %}

To prove Wachspress coordinates reproduce affine functions, he expresses the vector {% equation_inline p %} as a combination of neighbors using areas.
This long expression is the expanded form of that algebraic proof.

He likely said:

> “Don’t memorize this. Just understand: Wachspress coordinates satisfy linear precision because of the area-ratio construction.”

---

## **6. The “sum over j” formula**

I wrote:

```
sum(j, B_i / A_j*A_{j-1})(p-p_j)=0
```

This is shorthand for:

{% equation id="equilibrium-equation" %}
\sum_j w_j(p) \, (p - p_j) = 0
{% endequation %}

Which is the **equilibrium equation** for barycentric coordinates.
This is the key property: the weighted neighbors “pull” the point into equilibrium at (p).

---

## **7. MVC — Mean Value Coordinates (Floater)**

This was introduced because:

* cotangent weights have negative weights
* Wachspress works only for convex boundaries

MVC works on **any simple polygon**, convex or not.

### **Definition**

For each neighbor {% equation_inline j %}:

{% equation id="mvc-weights" %}
w_j(p) = \frac{\tan(\theta_{j-1}/2) + \tan(\theta_j/2)}{|p - p_j|}
{% endequation %}

Where {% equation_inline \theta_j %} is the angle between edges to {% equation_inline p_j %} and {% equation_inline p_{j+1} %}.

### **Properties**

**+ Always positive weights**
Works on non-convex polygons.

**+ Conformal-like behavior**
Angle-preserving tendencies (not exactly conformal, but close).

**– Not optimal for area distortion**
But usually much better than Tutte / springs.

Your professor might have said:

> “MVC behaves like a discrete harmonic map but without negative weights.”

---

## **8. Boundary conditions**

Barycentric coordinate systems typically require:

* at least 3 non-collinear boundary points to anchor the mapping (triangle boundary), or
* a fixed boundary polygon (arbitrary shape), or
* circular boundary (for Floater’s MVC disk embedding)

This is necessary to avoid the trivial solution {% equation_inline p_i = \text{constant} %}.

---

## **9. Most isometric parameterization (Lévy / Desbrun style)**


```
Most isometric Param
- start with a guess (fixed boundary)
- iteratively improve the "worst" point (intrinsic derivative)
```

This refers to **Most Isometric Parameterization (MIPS)** by Hormann & Greiner.

Key idea:

* Start with an initial mapping (e.g., MVC).
* Measure distortion at every triangle using singular values {% equation_inline \alpha_1, \alpha_2 %}.
* Improve the map by minimizing:

[
E = \frac{\alpha_1}{\alpha_2} + \frac{\alpha_2}{\alpha_1}
]

That is the “worst distortion” the professor refers to.

The process:

1. Fix the boundary (common choice).
2. Compute Jacobian on each triangle.
3. Compute singular values.
4. Move vertices to reduce the energy.
5. Iterate until distortion minimized.

---

## **10. Summary — what you should take away**

Your professor wants you to understand:

### **Cotangent weights**

* Best differential properties
* May become negative → dangerous in parametrization

### **Wachspress coordinates**

* Exact barycentric coordinates on convex polygons
* No negative weights
* Not general enough

### **Mean Value Coordinates (MVC)**

* General-purpose, always positive
* Great for disk parameterization
* Foundation for many modern methods (LSCM, MIPS)

### **General principle**

All these schemes boil down to:

{% equation id="general-barycentric" %}
p_i = \sum_j w_{ij} p_j, \qquad \sum_j w_{ij} = 1, \qquad w_{ij} \ge 0
{% endequation %}

The goal is to find weights that produce:

* good angle behavior
* no fold-overs
* smooth interior
* no negative weights
* linear precision

---


# Laboratory Sessions
## Lab 1: Normal approximation
We are given a point cloud we will call P. We assume it represents a surface (I dont know what will happen if it was a volume). Our purpose is to calculate the normal direction of the surface in that point. 

We will interpret a collection of points as a set of connected vertices of a surface. We will use Principal Component Analysis to obtain the 3 orthonormal axis that best represent the subset of point clouds.

  {% figure id="multiple-figures" size="1.0" 
  caption="Select a point, get its kNN points, calculate center of mass, move it to the origin"
  col="1" %}
  /images/gpr/ezgif-239f0d3b2d317a3f.gif
  {% endfigure %}

We can now create a matrix C that will contain how much each point differs from the origin. 

### Covariance matrix C

Formula (centered points):

{% equation id="energy" %}
C=\sum_{i=1}^{n}\tilde p_i\,\tilde p_i^{T}\quad\text{with}\quad \tilde p_i=p_i-\bar p,\;\bar p=\frac{1}{n}\sum_{i=1}^n p_i
{% endequation %}

Eigendecomposition:
{% equation id="energy" %}
C = V \,\Lambda\, V^{T}
{% endequation %}

where columns of {% equation_inline V %} are orthonormal eigenvectors and {% equation_inline \Lambda %} is the diagonal matrix of eigenvalues.

Short explanation:
- \(C\) is the (unnormalized) covariance / scatter matrix of the centered points.  
- It encodes how the points are distributed around their mean; principal directions (PCA) are given by the eigenvectors, and eigenvalues measure variance along those directions.

Numeric example (3 points in R^3):
- Points:
{% equation id="energy" %}
p_1=(1,0,0),\; p_2=(0,1,0),\; p_3=(0,0,1)
{% endequation %}
- Mean:
{% equation id="energy" %}
  \bar p=(\tfrac{1}{3},\tfrac{1}{3},\tfrac{1}{3})
{% endequation %}
- Centered vectors:
{% equation id="energy" %}
  \tilde p_1=(\tfrac{2}{3},-\tfrac{1}{3},-\tfrac{1}{3}),
  \tilde p_2=(-\tfrac{1}{3},\tfrac{2}{3},-\tfrac{1}{3}),
  \tilde p_3=(-\tfrac{1}{3},-\tfrac{1}{3},\tfrac{2}{3}).
{% endequation %}
- Compute {% equation_inline C=\sum \tilde p_i\tilde p_i^T %} :
{% equation id="energy" %}
  C=\begin{pmatrix}
  \tfrac{2}{3} & -\tfrac{1}{3} & -\tfrac{1}{3}\\[4pt]
  -\tfrac{1}{3} & \tfrac{2}{3} & -\tfrac{1}{3}\\[4pt]
  -\tfrac{1}{3} & -\tfrac{1}{3} & \tfrac{2}{3}
  \end{pmatrix}
{% endequation %}


 - Eigen-decomposition (PCA):
   - Eigenvalues: {% equation_inline \{1,\,1,\,0\} %}.
   - Orthonormal eigenvectors (columns of {% equation_inline V %}), for example:
     v1 = (1,-1,0)/√2, v2 = (1,1,-2)/√6, v3 = (1,1,1)/√3.
   - So {% equation_inline C = V\,\mathrm{diag}(1,1,0)\,V^{T} %}.

This shows \(C\) captures principal directions (two directions with variance 1, one null direction along the mean vector).

To calculate the normal direction,  we will need a special library capable of solving for {% equation_inline C=VΛV^T %}. We will use 


{% highlight cpp linenos %}
// We use Eigen's SelfAdjointEigenSolver to compute the eigenvalues and eigenvectors
Eigen::SelfAdjointEigenSolver<Eigen::Matrix3f> eigensolver(cov_eigen);
if (eigensolver.info() == Eigen::Success) {
  Eigen::Vector3f eigenvalues = eigensolver.eigenvalues();
  Eigen::Matrix3f eigenvectors = eigensolver.eigenvectors();
  
  // Smallest eigenvalue corresponds to normal direction
  // we know eigenvalues are sorted in increasing 
  Eigen::Vector3f normal = eigenvectors.col(0); // First column
  
  // Convert back to GLM
  normals[i] = glm::normalize(glm::vec3(normal.x(), normal.y(), normal.z()));
}
{% endhighlight %}

## Lab 2: Iterative Closest Point
## Lab 3: Reconstruction

We will generate a mesh from a point cloud. We are instructed two methods:
- Simple reconstruction
- Radial Basis FUnctions (RBF)

### Simple reconstruction
```python
i ← Index of pi, closest point to p
{ Compute z as the projection of p onto the tangent plane at pi }
z ← pi − ((p − pi) · ni) · ni
if distance(z, pi) ≤ ρ + δ then
    f (p) ← (p − pi) · ni
else
    f (p) ← undef ined
end if
```




### Hoppe Abstract
We describe and demonstrate an algorithm that takes as input an unorganized set of points {x1,...,xn} belonging to R³ on or near an unknown manifold M, and produces as output a simplicial surface that approximates M. Neither the topology, the presence of boundaries, nor the geometry of M are assumed to be known in advance — all are inferred automatically from the data. This problem naturally arises in a variety of practical situations such as range scanning an object from multiple view points, recovery of biological shapes from two-dimensional slices, and interactive surface sketching.

We get input: Set of pairs {% equation_inline (p_i, v_i), p_i ∈ R^3, v_i ∈ R %}.

{% equation_inline p_i %}  is just the sampled points in the surface (point cloud). {% equation_inline v_i %}  is a scalar obtained with {% equation_inline f(p) ← (p − p_i) · n_i %} 


Using the input (p_i, v_i) we compute

{% equation %}
f(p) = \sum_{i=1}^{m} f_i(p), \quad f_i(p) = \phi(\|p - p_i\|)\,c_i
{% endequation %}

Definitions:

- {% equation_inline \phi %} — radial basis kernel: a scalar function of the radial distance {% equation_inline r %} that controls smoothness and support. It depends only on {% equation_inline r = norm(p - p_i) %}. Common choices:
  - Gaussian: {% equation_inline \phi(r) = exp(-r^2 / (2 sigma^2)) %}
  - Multiquadric: {% equation_inline \phi(r) = sqrt(r^2 + eps^2) %}
  - Inverse multiquadric: {% equation_inline \phi(r) = 1 / sqrt(r^2 + eps^2) %}
  - Thin-plate spline: {% equation_inline \phi(r) = r^2 * log(r) %}
  - Compactly supported (Wendland): {% equation_inline \phi(r) = (1 - r / R)^k_+ * polynomial %}
  The choice determines smoothness, support radius and numerical conditioning.

- {% equation_inline norm(p - p_i) %} — Euclidean ({% equation_inline l2 %}) distance between query point {% equation_inline p \in R^3 %} and center {% equation_inline p_i \in R^3 %}:
  {% equation_inline r = norm(p - p_i)_2 %}.

- {% equation_inline c_i %} — scalar coefficient (weight) associated to center {% equation_inline p_i %}. Coefficients are solved so {% equation_inline f %} interpolates or approximates the data:
  - Assemble {% equation_inline A \in R^{m x m} %} with {% equation_inline A_{ij} = \phi(norm(p_i - p_j)) %}.
  - Solve {% equation_inline A c = v %} for {% equation_inline c %} (or regularized: {% equation_inline (A + \lambda I) c = v %}) where {% equation_inline c = [c_1 ... c_m]^T %} and {% equation_inline v = [v_1 ... v_m]^T %}.

- {% equation_inline p_i %} — centers (sample positions) in {% equation_inline R^3 %}. Typically {% equation_inline m = n %} and centers coincide with the input samples, but you may choose a subset of centers.

- {% equation_inline v_i %} — target scalar values at centers (observations). In surface reconstruction from point+normal data these are often:
  - {% equation_inline v_i = 0 %} for on-surface samples,
  - additional synthetic off-surface samples {% equation_inline v_i = +- d %} (offset by normal) to impose normal constraints.
  More generally {% equation_inline v_i %} is the desired value {% equation_inline f(p_i) %}.

Notes:
- After solving for {% equation_inline c %}, evaluate {% equation_inline f(p) %} cheaply by summing {% equation_inline \phi(norm(p - p_i)) * c_i %}.
- Regularization ({% equation_inline lambda > 0 %}) improves conditioning for large {% equation_inline m %} or noisy data.
- Choosing compactly supported {% equation_inline \phi %} makes {% equation_inline A %} sparse and evaluation faster.
As m increases, the matrix A tends to become ill-conditioned. "Ill-conditioned" means the mathematical problem (typically a linear system or a matrix) is highly sensitive to small changes in the input data or to rounding errors. In practice:

- The determinant of A is close to zero and the condition number is very large.
- Solving A x = b becomes unstable: tiny changes in A or b can produce huge changes in x.
- This commonly occurs when the sample points (or basis functions) are nearly linearly dependent, are very close together, or when the system is near singular.

(Regularization such as solving (A + λI) c = v is a common mitigation.)

<!-- You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-title.MARKUP`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `MARKUP` is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:

{% highlight ruby %}
def print_hi(name)
  puts "Hi, #{name}"
end
print_hi('Tom')
#=> prints 'Hi, Tom' to STDOUT.
{% endhighlight %}

Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/ -->
